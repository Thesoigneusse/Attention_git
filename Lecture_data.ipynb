{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlayers\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmatrice\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(crt\u001b[38;5;241m.\u001b[39mtokens))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mtokens) \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m ctxs]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "print(layers[0][0].matrice.size())\n",
    "print(len(crt.tokens))\n",
    "print(sum([len(ctx.tokens) for ctx in ctxs]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[36, 44, 33]\n"
     ]
    }
   ],
   "source": [
    "print(len(crt.tokens))\n",
    "print([len(ctx.tokens) for ctx in ctxs])\n",
    "test = Matrice(layers[0][0].matrice[-len(crt.tokens):, -len(crt.tokens):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification des structures pour la lecture du contextes et de l'obtention des chaines de  coréférences correspondantes\n",
    "Modification de la lecture des phrases de contextes et de la lecture des matrices\n",
    "Concaténation des matrices sur les lignes\n",
    "Modification des éléments passés pour l'alignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['&quot;', 'I', '&apos;m', 'not', 'exag@@', 'ger@@', 'ating', 'when', 'I', 'say', 'that', 'every', 'woman', 'who', 'arri@@', 'ves', 'here', 'has', 'dealt', 'with', 'sexual', 'violence', 'on', 'her', 'way', 'to', 'find', 'refu@@', 'ge', ',', '&quot;', 'said', 'B@@', 'ahr', '.', '<eos>'], ['&quot;', 'Men', 'fle@@', 'e', 'because', 'of', 'war', ',', 'because', 'they', 'are', 'politically', 'persec@@', 'uted', ',', 'because', 'they', 'are', 'threatened', 'with', 'torture', 'or', 'death', ',', 'or', 'because', 'their', 'families', 'pin', 'their', 'hopes', 'on', 'them', 'and', 'send', 'them', 'to', 'Europe', ',', '&quot;', 'she', 'explains', '.', '<eos>'], ['When', 'women', 'fle@@', 'e', ',', 'other', 'reasons', 'are', 'in', 'the', 'fore@@', 'ground', ':', '&quot;', 'Many', 'women', 's@@', 'ne@@', 'ak', 'away', 'secre@@', 'tly', ',', 'because', 'they', 'see', 'no', 'other', 'way', 'out', '.', '&quot;', '<eos>'], ['In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<END>']]\n"
     ]
    }
   ],
   "source": [
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'identifiant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMatrice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mecriture_xslx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/getalp/lopezfab/Documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_matrice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bureau/Attention3/Matrice.py:286\u001b[0m, in \u001b[0;36mMatrice.ecriture_xslx\u001b[0;34m(self, crt, ctx, absolute_folder, filename, precision, create_folder_path)\u001b[0m\n\u001b[1;32m    283\u001b[0m highlight_format \u001b[38;5;241m=\u001b[39m workbook\u001b[38;5;241m.\u001b[39madd_format({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbg_color\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcyan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Ecritures des phrases respectivement courante et de contexte\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m worksheet\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrt\u001b[38;5;241m.\u001b[39midentifiant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(crt\u001b[38;5;241m.\u001b[39midentifiant)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mint\u001b[39m(ctx\u001b[38;5;241m.\u001b[39midentifiant))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_idx, tok \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(crt\u001b[38;5;241m.\u001b[39mtokens, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    288\u001b[0m     worksheet\u001b[38;5;241m.\u001b[39mwrite(row_idx, \u001b[38;5;241m0\u001b[39m, tok)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'identifiant'"
     ]
    }
   ],
   "source": [
    "Matrice.ecriture_xslx(layers[0][0], src, src, \"/home/getalp/lopezfab/Documents\", \"full_matrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Doctest clear\n",
      "dict_keys(['id', 'src_tokens', 'src_segments_labels', 'tgt_tokens', 'tgt_eos_pos', 'enc_attn', 'dec_attn', 'heads_enc_attn', 'heads_dec_attn'])\n",
      "[DEBUG] len(full_sentence): 138\n",
      "[[{'_matrice': tensor([[0.01, 0.01, 0.00,  ..., 0.01, 0.01, 0.02],\n",
      "        [0.03, 0.02, 0.01,  ..., 0.01, 0.01, 0.01],\n",
      "        [0.08, 0.10, 0.04,  ..., 0.00, 0.00, 0.00],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.06, 0.02, 0.02],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.14, 0.09, 0.08],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.19, 0.09, 0.11]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.19e-03, 5.78e-02, 5.02e-03,  ..., 9.28e-04, 2.34e-03, 1.09e-03],\n",
      "        [2.43e-01, 1.38e-02, 2.84e-02,  ..., 5.94e-03, 9.71e-04, 4.67e-04],\n",
      "        [5.62e-03, 9.57e-01, 5.48e-03,  ..., 7.83e-05, 1.87e-06, 5.20e-07],\n",
      "        ...,\n",
      "        [1.70e-07, 6.97e-08, 4.99e-07,  ..., 3.06e-02, 5.04e-04, 9.87e-07],\n",
      "        [3.83e-07, 8.63e-08, 1.38e-07,  ..., 9.95e-01, 1.86e-04, 5.28e-05],\n",
      "        [1.46e-07, 2.30e-08, 1.25e-07,  ..., 1.06e-02, 7.22e-01, 2.67e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.69e-04, 3.44e-04, 3.90e-05,  ..., 1.11e-03, 6.12e-03, 1.25e-02],\n",
      "        [1.17e-04, 2.67e-04, 6.91e-05,  ..., 2.40e-03, 1.28e-02, 3.15e-02],\n",
      "        [9.06e-05, 2.49e-04, 1.14e-04,  ..., 5.82e-03, 1.20e-02, 3.00e-02],\n",
      "        ...,\n",
      "        [1.87e-03, 4.68e-03, 5.30e-03,  ..., 9.86e-02, 1.76e-01, 2.64e-01],\n",
      "        [8.70e-03, 2.02e-02, 7.58e-03,  ..., 8.64e-02, 2.13e-01, 2.02e-01],\n",
      "        [4.01e-02, 3.35e-02, 3.66e-02,  ..., 9.88e-02, 5.55e-02, 2.66e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.39e-03, 2.66e-02, 8.46e-02,  ..., 5.87e-03, 1.75e-03, 4.74e-03],\n",
      "        [5.67e-03, 4.96e-03, 1.07e-01,  ..., 5.31e-03, 2.21e-03, 1.42e-03],\n",
      "        [3.12e-03, 1.66e-02, 5.14e-02,  ..., 1.15e-03, 1.11e-03, 4.31e-04],\n",
      "        ...,\n",
      "        [8.99e-04, 2.61e-03, 4.44e-03,  ..., 8.00e-02, 2.18e-01, 2.98e-01],\n",
      "        [1.03e-05, 5.26e-05, 3.57e-05,  ..., 4.53e-03, 4.76e-03, 9.77e-01],\n",
      "        [8.14e-05, 3.52e-05, 5.78e-06,  ..., 1.92e-03, 2.50e-02, 9.35e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.00, 0.01, 0.02,  ..., 0.01, 0.00, 0.01],\n",
      "        [0.02, 0.00, 0.01,  ..., 0.02, 0.01, 0.00],\n",
      "        [0.00, 0.00, 0.02,  ..., 0.02, 0.00, 0.01],\n",
      "        ...,\n",
      "        [0.01, 0.00, 0.01,  ..., 0.15, 0.01, 0.03],\n",
      "        [0.03, 0.00, 0.02,  ..., 0.15, 0.01, 0.02],\n",
      "        [0.03, 0.00, 0.01,  ..., 0.05, 0.01, 0.02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.00, 0.01, 0.00,  ..., 0.01, 0.00, 0.00],\n",
      "        [0.00, 0.01, 0.01,  ..., 0.00, 0.00, 0.00],\n",
      "        [0.00, 0.01, 0.00,  ..., 0.00, 0.00, 0.00],\n",
      "        ...,\n",
      "        [0.01, 0.00, 0.00,  ..., 0.01, 0.00, 0.00],\n",
      "        [0.02, 0.03, 0.02,  ..., 0.00, 0.01, 0.01],\n",
      "        [0.05, 0.02, 0.01,  ..., 0.01, 0.01, 0.01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[9.43e-01, 2.19e-02, 2.99e-03,  ..., 7.41e-07, 6.80e-06, 7.92e-07],\n",
      "        [2.16e-01, 2.27e-01, 2.42e-02,  ..., 3.14e-05, 2.19e-04, 3.58e-05],\n",
      "        [9.80e-02, 8.42e-02, 3.67e-01,  ..., 6.86e-04, 5.40e-04, 1.77e-04],\n",
      "        ...,\n",
      "        [2.60e-07, 1.43e-06, 7.62e-06,  ..., 9.41e-01, 5.97e-04, 1.09e-03],\n",
      "        [1.93e-03, 9.98e-03, 1.23e-03,  ..., 2.24e-03, 1.25e-01, 1.84e-02],\n",
      "        [1.84e-06, 3.31e-05, 1.84e-05,  ..., 7.12e-03, 9.55e-02, 3.99e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[9.53e-03, 8.80e-02, 9.63e-02,  ..., 8.79e-03, 1.57e-03, 6.92e-03],\n",
      "        [1.58e-02, 6.64e-03, 2.82e-01,  ..., 1.25e-02, 7.95e-04, 4.50e-03],\n",
      "        [1.86e-02, 2.44e-02, 1.43e-01,  ..., 7.16e-03, 4.08e-04, 7.77e-04],\n",
      "        ...,\n",
      "        [3.30e-04, 1.81e-04, 3.30e-04,  ..., 9.36e-01, 2.18e-02, 1.05e-02],\n",
      "        [7.42e-04, 6.47e-04, 1.51e-03,  ..., 9.81e-02, 8.40e-03, 7.22e-01],\n",
      "        [1.92e-04, 2.92e-05, 2.80e-04,  ..., 1.15e-01, 4.96e-02, 2.29e-01]]), '_normalized': False, '_suppr_inf': False}], [{'_matrice': tensor([[4.24e-01, 9.43e-02, 1.29e-01,  ..., 8.92e-05, 3.29e-04, 1.27e-04],\n",
      "        [1.28e-01, 5.35e-01, 1.07e-01,  ..., 1.04e-04, 3.33e-04, 1.68e-04],\n",
      "        [7.50e-03, 1.85e-02, 7.10e-01,  ..., 2.19e-04, 1.09e-03, 5.08e-04],\n",
      "        ...,\n",
      "        [1.10e-03, 5.60e-04, 2.50e-04,  ..., 7.61e-01, 3.66e-03, 3.33e-03],\n",
      "        [2.10e-03, 2.94e-03, 3.01e-03,  ..., 1.52e-01, 8.31e-02, 4.11e-02],\n",
      "        [3.33e-03, 4.66e-03, 1.44e-03,  ..., 2.44e-01, 2.07e-02, 2.71e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.03, 0.01, 0.02,  ..., 0.00, 0.00, 0.00],\n",
      "        [0.00, 0.02, 0.01,  ..., 0.00, 0.00, 0.00],\n",
      "        [0.00, 0.01, 0.02,  ..., 0.00, 0.00, 0.00],\n",
      "        ...,\n",
      "        [0.01, 0.00, 0.01,  ..., 0.04, 0.05, 0.06],\n",
      "        [0.00, 0.00, 0.01,  ..., 0.01, 0.02, 0.02],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.02, 0.03, 0.04]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.65e-03, 2.97e-02, 3.54e-02,  ..., 8.07e-03, 8.56e-03, 3.46e-03],\n",
      "        [8.37e-02, 3.26e-04, 7.43e-03,  ..., 1.04e-03, 6.05e-04, 3.26e-04],\n",
      "        [9.49e-03, 3.03e-03, 2.62e-03,  ..., 3.72e-03, 1.60e-02, 3.68e-03],\n",
      "        ...,\n",
      "        [2.14e-03, 3.13e-03, 8.27e-03,  ..., 1.01e-01, 1.07e-02, 2.46e-02],\n",
      "        [8.13e-02, 2.46e-02, 1.23e-01,  ..., 1.49e-03, 1.51e-03, 1.37e-03],\n",
      "        [4.86e-02, 5.47e-02, 3.29e-01,  ..., 5.09e-04, 1.62e-03, 8.21e-04]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[9.53e-03, 9.73e-01, 1.45e-02,  ..., 1.83e-07, 6.16e-06, 1.04e-05],\n",
      "        [5.85e-03, 5.40e-03, 6.99e-01,  ..., 9.77e-08, 7.58e-07, 5.48e-07],\n",
      "        [4.41e-06, 3.75e-04, 8.11e-03,  ..., 9.86e-08, 7.52e-07, 9.21e-07],\n",
      "        ...,\n",
      "        [4.41e-06, 1.20e-05, 1.40e-06,  ..., 1.13e-01, 6.32e-01, 2.45e-01],\n",
      "        [1.94e-04, 7.41e-04, 3.47e-04,  ..., 3.07e-02, 1.83e-01, 6.39e-01],\n",
      "        [3.19e-04, 3.11e-04, 3.02e-04,  ..., 2.41e-02, 2.23e-01, 2.88e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.22, 0.01, 0.02,  ..., 0.00, 0.01, 0.01],\n",
      "        [0.05, 0.10, 0.08,  ..., 0.00, 0.01, 0.01],\n",
      "        [0.13, 0.08, 0.12,  ..., 0.00, 0.02, 0.02],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.01, 0.02, 0.02],\n",
      "        [0.01, 0.01, 0.01,  ..., 0.02, 0.03, 0.03],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.04, 0.03, 0.03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.35e-01, 1.52e-01, 1.92e-03,  ..., 1.34e-03, 2.06e-02, 4.68e-03],\n",
      "        [9.48e-01, 2.25e-02, 2.52e-03,  ..., 3.02e-05, 3.46e-04, 5.37e-05],\n",
      "        [1.49e-01, 7.64e-01, 4.20e-02,  ..., 1.16e-05, 3.27e-04, 3.09e-05],\n",
      "        ...,\n",
      "        [7.15e-06, 9.69e-06, 1.48e-05,  ..., 9.33e-02, 8.71e-03, 2.92e-03],\n",
      "        [3.10e-03, 1.82e-03, 2.07e-03,  ..., 2.52e-01, 9.20e-02, 9.53e-02],\n",
      "        [1.41e-04, 9.07e-05, 2.02e-04,  ..., 6.10e-01, 8.43e-02, 8.42e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.01, 0.00, 0.01,  ..., 0.01, 0.02, 0.02],\n",
      "        [0.01, 0.01, 0.01,  ..., 0.02, 0.01, 0.01],\n",
      "        [0.01, 0.01, 0.01,  ..., 0.01, 0.02, 0.01],\n",
      "        ...,\n",
      "        [0.01, 0.00, 0.01,  ..., 0.01, 0.04, 0.03],\n",
      "        [0.03, 0.00, 0.01,  ..., 0.01, 0.06, 0.03],\n",
      "        [0.02, 0.00, 0.00,  ..., 0.02, 0.06, 0.02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[7.21e-02, 6.51e-02, 8.08e-02,  ..., 1.02e-03, 1.01e-02, 5.45e-03],\n",
      "        [8.90e-02, 2.35e-02, 2.07e-01,  ..., 5.33e-04, 4.18e-03, 2.03e-03],\n",
      "        [2.62e-02, 1.90e-02, 7.48e-02,  ..., 7.88e-04, 1.98e-03, 1.59e-03],\n",
      "        ...,\n",
      "        [1.02e-03, 2.07e-04, 7.55e-04,  ..., 9.04e-01, 3.19e-03, 3.64e-03],\n",
      "        [1.37e-02, 1.25e-01, 1.38e-01,  ..., 8.32e-04, 1.45e-03, 1.74e-03],\n",
      "        [1.07e-02, 7.00e-02, 1.65e-01,  ..., 1.80e-03, 3.98e-03, 5.05e-03]]), '_normalized': False, '_suppr_inf': False}], [{'_matrice': tensor([[6.99e-03, 9.03e-03, 3.36e-03,  ..., 5.19e-05, 5.97e-03, 2.85e-03],\n",
      "        [5.46e-02, 1.46e-02, 4.66e-02,  ..., 1.46e-04, 4.14e-03, 2.19e-03],\n",
      "        [2.80e-02, 2.10e-01, 5.12e-02,  ..., 5.67e-05, 3.13e-03, 9.03e-04],\n",
      "        ...,\n",
      "        [8.31e-07, 7.55e-07, 3.88e-07,  ..., 3.06e-04, 9.88e-04, 7.59e-04],\n",
      "        [6.99e-05, 3.33e-05, 5.18e-05,  ..., 1.86e-03, 9.10e-03, 8.48e-03],\n",
      "        [2.99e-05, 8.00e-06, 9.33e-06,  ..., 2.31e-03, 3.79e-03, 2.65e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.37e-01, 1.39e-01, 1.12e-01,  ..., 4.77e-05, 1.58e-04, 1.82e-04],\n",
      "        [3.41e-02, 1.89e-02, 2.04e-02,  ..., 3.91e-04, 5.69e-04, 4.69e-04],\n",
      "        [5.00e-03, 8.33e-03, 3.70e-03,  ..., 6.38e-04, 3.38e-04, 2.08e-04],\n",
      "        ...,\n",
      "        [2.18e-04, 3.80e-04, 1.23e-04,  ..., 1.59e-02, 4.89e-02, 9.18e-02],\n",
      "        [1.33e-03, 7.21e-04, 5.62e-04,  ..., 3.26e-03, 1.64e-02, 2.09e-02],\n",
      "        [2.74e-04, 1.43e-04, 1.20e-04,  ..., 1.17e-03, 7.97e-03, 9.87e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[8.04e-02, 9.53e-03, 1.30e-02,  ..., 1.07e-03, 2.33e-02, 1.57e-01],\n",
      "        [5.09e-03, 3.77e-02, 1.09e-02,  ..., 9.04e-04, 1.19e-02, 3.22e-02],\n",
      "        [1.25e-02, 1.86e-02, 8.48e-03,  ..., 7.09e-04, 2.80e-02, 3.27e-02],\n",
      "        ...,\n",
      "        [5.89e-05, 1.77e-04, 6.22e-05,  ..., 6.96e-04, 8.12e-03, 1.01e-02],\n",
      "        [2.76e-02, 5.43e-03, 2.33e-02,  ..., 4.95e-04, 8.75e-04, 4.06e-02],\n",
      "        [1.73e-02, 2.83e-03, 1.07e-02,  ..., 1.01e-03, 1.34e-02, 5.79e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[7.40e-01, 5.23e-02, 3.04e-03,  ..., 4.61e-04, 1.31e-03, 6.04e-04],\n",
      "        [2.95e-01, 2.98e-01, 9.88e-03,  ..., 1.12e-03, 3.86e-04, 2.37e-04],\n",
      "        [3.14e-02, 7.77e-02, 1.49e-01,  ..., 2.61e-03, 1.33e-03, 3.54e-04],\n",
      "        ...,\n",
      "        [2.67e-04, 6.36e-04, 1.10e-04,  ..., 4.65e-01, 3.85e-02, 4.79e-02],\n",
      "        [1.07e-03, 2.42e-03, 6.29e-04,  ..., 6.41e-02, 1.55e-01, 1.15e-01],\n",
      "        [2.38e-03, 2.56e-03, 9.10e-04,  ..., 7.93e-02, 1.05e-01, 2.30e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.05, 0.05, 0.02,  ..., 0.01, 0.02, 0.01],\n",
      "        [0.03, 0.03, 0.02,  ..., 0.01, 0.01, 0.01],\n",
      "        [0.03, 0.02, 0.02,  ..., 0.01, 0.01, 0.01],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.05, 0.07, 0.05],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.01, 0.04, 0.02],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.02, 0.05, 0.03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[5.39e-03, 2.93e-03, 2.39e-03,  ..., 4.69e-06, 1.40e-04, 1.97e-04],\n",
      "        [1.38e-02, 9.38e-04, 7.72e-04,  ..., 5.11e-05, 1.77e-04, 4.28e-04],\n",
      "        [5.55e-03, 5.64e-03, 1.28e-03,  ..., 6.46e-05, 1.91e-03, 6.72e-04],\n",
      "        ...,\n",
      "        [2.84e-06, 2.15e-06, 7.83e-07,  ..., 3.70e-03, 2.25e-03, 2.47e-03],\n",
      "        [2.35e-03, 8.89e-04, 3.39e-04,  ..., 1.05e-02, 2.49e-02, 1.51e-02],\n",
      "        [4.53e-06, 1.69e-06, 2.71e-07,  ..., 9.91e-05, 9.86e-01, 1.25e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.43e-03, 9.96e-01, 2.46e-04,  ..., 1.45e-08, 4.52e-08, 1.27e-07],\n",
      "        [2.14e-02, 1.29e-02, 7.67e-01,  ..., 8.99e-06, 2.58e-06, 3.35e-06],\n",
      "        [1.16e-05, 2.04e-05, 3.78e-05,  ..., 1.38e-05, 1.21e-06, 1.05e-06],\n",
      "        ...,\n",
      "        [3.37e-08, 6.06e-09, 1.03e-09,  ..., 4.55e-04, 2.84e-01, 5.51e-01],\n",
      "        [1.14e-05, 1.85e-06, 9.17e-07,  ..., 1.05e-04, 1.43e-02, 4.50e-02],\n",
      "        [2.67e-06, 1.96e-07, 8.94e-08,  ..., 5.98e-05, 7.85e-03, 2.03e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.14e-02, 2.86e-02, 1.23e-02,  ..., 7.24e-05, 1.72e-02, 9.49e-03],\n",
      "        [1.76e-01, 1.96e-02, 7.11e-03,  ..., 1.14e-04, 9.32e-03, 8.84e-03],\n",
      "        [2.16e-01, 2.65e-02, 5.60e-03,  ..., 8.41e-05, 5.17e-03, 4.63e-03],\n",
      "        ...,\n",
      "        [2.83e-04, 2.00e-04, 3.61e-04,  ..., 2.10e-02, 1.64e-02, 2.35e-02],\n",
      "        [4.17e-03, 2.27e-03, 2.84e-03,  ..., 1.23e-02, 1.50e-02, 1.56e-02],\n",
      "        [7.61e-04, 5.71e-04, 7.71e-04,  ..., 8.97e-03, 4.13e-02, 3.72e-02]]), '_normalized': False, '_suppr_inf': False}], [{'_matrice': tensor([[0.12, 0.09, 0.03,  ..., 0.00, 0.01, 0.01],\n",
      "        [0.07, 0.15, 0.03,  ..., 0.00, 0.01, 0.01],\n",
      "        [0.09, 0.11, 0.01,  ..., 0.00, 0.01, 0.00],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.06, 0.01, 0.03],\n",
      "        [0.01, 0.01, 0.00,  ..., 0.01, 0.03, 0.04],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.02, 0.03, 0.03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.07e-03, 7.39e-03, 1.70e-03,  ..., 1.13e-04, 5.07e-01, 4.83e-02],\n",
      "        [8.31e-03, 4.08e-03, 4.55e-02,  ..., 2.12e-03, 6.45e-02, 5.93e-02],\n",
      "        [4.84e-03, 5.89e-03, 1.19e-02,  ..., 1.69e-03, 1.35e-01, 4.99e-02],\n",
      "        ...,\n",
      "        [2.42e-05, 1.45e-03, 4.32e-04,  ..., 1.15e-02, 1.74e-02, 4.95e-02],\n",
      "        [1.68e-03, 2.43e-03, 4.96e-03,  ..., 1.18e-03, 3.71e-02, 4.88e-02],\n",
      "        [2.36e-04, 7.27e-04, 4.71e-04,  ..., 6.78e-04, 4.15e-01, 4.93e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[9.19e-04, 2.73e-03, 1.69e-03,  ..., 2.52e-04, 2.48e-03, 3.09e-04],\n",
      "        [8.03e-03, 2.70e-03, 2.10e-02,  ..., 8.65e-04, 5.75e-03, 6.51e-04],\n",
      "        [3.13e-03, 4.62e-03, 3.62e-03,  ..., 1.34e-04, 8.45e-04, 8.36e-05],\n",
      "        ...,\n",
      "        [6.60e-05, 4.04e-05, 6.03e-05,  ..., 3.93e-03, 4.49e-03, 5.86e-03],\n",
      "        [4.08e-03, 4.39e-03, 1.71e-02,  ..., 9.23e-03, 1.47e-02, 5.77e-03],\n",
      "        [3.67e-04, 5.17e-04, 4.12e-04,  ..., 5.27e-03, 1.03e-02, 3.18e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.32e-01, 3.31e-02, 1.61e-02,  ..., 2.15e-03, 6.62e-03, 2.78e-03],\n",
      "        [4.50e-01, 1.51e-02, 8.61e-03,  ..., 6.53e-04, 2.88e-03, 1.77e-03],\n",
      "        [5.47e-01, 3.69e-02, 6.71e-03,  ..., 9.63e-04, 2.12e-03, 1.29e-03],\n",
      "        ...,\n",
      "        [1.33e-03, 1.02e-03, 1.26e-03,  ..., 9.12e-03, 1.52e-02, 2.06e-02],\n",
      "        [1.43e-02, 2.99e-03, 2.78e-03,  ..., 1.32e-02, 1.32e-01, 3.26e-02],\n",
      "        [1.33e-03, 2.60e-04, 8.70e-05,  ..., 1.59e-02, 6.81e-01, 1.30e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.14, 0.02, 0.02,  ..., 0.00, 0.02, 0.02],\n",
      "        [0.07, 0.08, 0.02,  ..., 0.00, 0.02, 0.01],\n",
      "        [0.04, 0.06, 0.10,  ..., 0.00, 0.01, 0.01],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.14, 0.01, 0.05],\n",
      "        [0.02, 0.01, 0.01,  ..., 0.03, 0.03, 0.06],\n",
      "        [0.02, 0.00, 0.00,  ..., 0.05, 0.04, 0.16]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[7.31e-03, 2.43e-03, 2.67e-03,  ..., 7.97e-04, 2.13e-03, 1.27e-03],\n",
      "        [6.88e-02, 5.41e-03, 1.83e-01,  ..., 3.07e-04, 1.74e-03, 1.35e-03],\n",
      "        [3.39e-03, 2.88e-03, 1.60e-02,  ..., 2.10e-04, 1.32e-03, 4.60e-04],\n",
      "        ...,\n",
      "        [3.68e-05, 6.99e-06, 9.05e-06,  ..., 2.52e-02, 1.05e-02, 1.86e-03],\n",
      "        [6.94e-03, 3.00e-03, 3.11e-03,  ..., 1.85e-02, 9.22e-03, 1.32e-02],\n",
      "        [3.59e-04, 2.75e-04, 1.71e-04,  ..., 1.11e-02, 3.97e-03, 1.96e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.66e-03, 7.45e-03, 6.59e-03,  ..., 5.37e-04, 9.06e-04, 8.92e-04],\n",
      "        [3.32e-03, 2.17e-03, 1.01e-02,  ..., 3.68e-04, 6.84e-04, 3.49e-04],\n",
      "        [5.69e-03, 1.34e-03, 3.11e-03,  ..., 9.40e-04, 6.72e-04, 4.64e-04],\n",
      "        ...,\n",
      "        [2.46e-05, 3.89e-05, 6.37e-06,  ..., 5.56e-03, 5.07e-03, 3.51e-03],\n",
      "        [2.75e-03, 9.82e-04, 5.99e-04,  ..., 3.21e-03, 3.52e-03, 3.08e-03],\n",
      "        [3.19e-04, 5.39e-05, 2.39e-05,  ..., 2.26e-03, 4.02e-03, 1.95e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.40e-03, 1.62e-04, 1.05e-05,  ..., 1.66e-07, 1.33e-01, 6.99e-04],\n",
      "        [9.84e-01, 3.56e-06, 1.70e-07,  ..., 1.80e-07, 1.58e-05, 6.40e-07],\n",
      "        [2.42e-03, 9.94e-01, 4.83e-04,  ..., 1.15e-07, 1.32e-05, 6.28e-07],\n",
      "        ...,\n",
      "        [7.88e-12, 1.40e-11, 1.61e-09,  ..., 1.25e-05, 1.75e-07, 7.98e-07],\n",
      "        [3.45e-04, 1.08e-05, 2.34e-05,  ..., 6.56e-01, 7.30e-03, 1.06e-02],\n",
      "        [1.50e-05, 1.25e-06, 3.82e-07,  ..., 8.84e-01, 2.09e-03, 2.53e-03]]), '_normalized': False, '_suppr_inf': False}], [{'_matrice': tensor([[6.19e-02, 6.78e-03, 1.82e-03,  ..., 4.79e-05, 1.12e-01, 9.61e-04],\n",
      "        [2.42e-02, 9.75e-03, 2.54e-03,  ..., 8.71e-05, 1.18e-01, 8.83e-04],\n",
      "        [3.24e-02, 9.04e-03, 2.39e-03,  ..., 3.16e-05, 1.18e-01, 4.11e-04],\n",
      "        ...,\n",
      "        [3.40e-04, 2.98e-04, 3.30e-05,  ..., 6.31e-03, 1.22e-01, 1.70e-02],\n",
      "        [5.67e-03, 3.87e-03, 5.39e-03,  ..., 4.30e-02, 5.39e-03, 2.06e-02],\n",
      "        [6.76e-03, 1.27e-03, 2.47e-04,  ..., 1.08e-03, 1.09e-01, 9.39e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.12, 0.01, 0.00,  ..., 0.00, 0.06, 0.02],\n",
      "        [0.16, 0.02, 0.01,  ..., 0.00, 0.04, 0.02],\n",
      "        [0.12, 0.03, 0.01,  ..., 0.00, 0.03, 0.01],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.02, 0.09, 0.02],\n",
      "        [0.01, 0.02, 0.01,  ..., 0.01, 0.00, 0.01],\n",
      "        [0.01, 0.00, 0.00,  ..., 0.02, 0.08, 0.03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.14e-02, 2.31e-02, 8.29e-03,  ..., 2.27e-04, 1.14e-01, 3.28e-03],\n",
      "        [4.62e-01, 2.14e-02, 3.52e-03,  ..., 7.29e-06, 7.12e-02, 6.42e-04],\n",
      "        [5.83e-01, 9.48e-02, 1.58e-02,  ..., 3.07e-06, 3.98e-02, 3.86e-04],\n",
      "        ...,\n",
      "        [1.35e-05, 7.78e-06, 5.90e-06,  ..., 3.51e-03, 2.75e-04, 4.93e-03],\n",
      "        [2.11e-04, 2.79e-04, 2.10e-04,  ..., 5.78e-02, 4.17e-03, 3.23e-02],\n",
      "        [7.51e-04, 2.75e-04, 1.35e-04,  ..., 1.85e-01, 8.55e-03, 3.63e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.89e-02, 4.39e-01, 2.08e-01,  ..., 3.64e-05, 1.04e-02, 1.03e-03],\n",
      "        [3.43e-02, 8.29e-03, 1.97e-01,  ..., 1.03e-04, 3.75e-03, 2.72e-04],\n",
      "        [1.11e-03, 7.10e-04, 8.10e-03,  ..., 1.14e-05, 5.29e-04, 3.39e-05],\n",
      "        ...,\n",
      "        [5.74e-05, 9.87e-06, 6.80e-06,  ..., 2.14e-02, 1.14e-01, 1.82e-01],\n",
      "        [8.92e-03, 7.02e-03, 6.99e-03,  ..., 1.13e-03, 3.57e-03, 6.45e-04],\n",
      "        [1.70e-04, 1.93e-04, 7.25e-05,  ..., 3.86e-03, 1.29e-01, 7.68e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.82e-03, 1.84e-02, 1.84e-02,  ..., 4.93e-05, 1.09e-01, 1.88e-04],\n",
      "        [8.78e-03, 3.26e-02, 2.01e-01,  ..., 3.89e-04, 5.06e-02, 3.80e-03],\n",
      "        [5.73e-03, 1.95e-01, 1.18e-01,  ..., 8.00e-05, 2.75e-02, 5.10e-04],\n",
      "        ...,\n",
      "        [1.44e-04, 1.19e-04, 7.50e-05,  ..., 5.03e-03, 5.44e-02, 1.82e-02],\n",
      "        [1.28e-02, 7.69e-03, 6.44e-03,  ..., 2.25e-03, 1.22e-03, 1.56e-03],\n",
      "        [3.04e-04, 2.85e-04, 2.41e-04,  ..., 2.04e-03, 1.18e-01, 2.80e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.89e-04, 1.36e-04, 4.82e-05,  ..., 1.05e-04, 1.32e-01, 5.86e-04],\n",
      "        [1.69e-02, 6.17e-04, 1.01e-03,  ..., 9.32e-04, 1.10e-01, 1.38e-03],\n",
      "        [7.78e-03, 2.67e-04, 1.82e-03,  ..., 7.21e-04, 9.32e-02, 2.10e-04],\n",
      "        ...,\n",
      "        [3.21e-04, 7.45e-04, 6.58e-05,  ..., 8.05e-02, 8.49e-02, 5.40e-02],\n",
      "        [1.23e-02, 7.11e-03, 4.19e-03,  ..., 6.31e-03, 2.98e-02, 1.15e-02],\n",
      "        [9.90e-05, 4.44e-05, 2.51e-05,  ..., 4.28e-03, 1.26e-01, 4.09e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[4.72e-02, 5.25e-03, 1.77e-03,  ..., 1.01e-04, 5.27e-02, 7.69e-03],\n",
      "        [1.27e-02, 1.40e-02, 5.21e-03,  ..., 1.05e-03, 4.62e-02, 8.80e-03],\n",
      "        [2.00e-03, 5.70e-03, 1.38e-03,  ..., 4.79e-04, 6.02e-02, 4.23e-03],\n",
      "        ...,\n",
      "        [5.64e-05, 4.39e-05, 1.20e-05,  ..., 1.08e-02, 1.28e-01, 3.77e-03],\n",
      "        [6.07e-03, 3.55e-03, 5.50e-03,  ..., 7.73e-03, 3.81e-03, 2.09e-02],\n",
      "        [5.32e-04, 3.98e-04, 2.38e-04,  ..., 2.68e-03, 1.20e-01, 8.26e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.05e-01, 5.83e-02, 3.85e-02,  ..., 4.19e-03, 1.66e-02, 4.97e-03],\n",
      "        [2.37e-01, 3.32e-01, 4.04e-02,  ..., 2.43e-03, 6.62e-03, 2.31e-03],\n",
      "        [1.20e-01, 6.98e-02, 2.64e-01,  ..., 1.93e-03, 7.40e-03, 3.35e-03],\n",
      "        ...,\n",
      "        [1.75e-04, 5.10e-05, 3.64e-05,  ..., 3.83e-01, 2.51e-02, 4.77e-02],\n",
      "        [3.67e-03, 3.35e-03, 3.70e-03,  ..., 1.65e-02, 2.62e-02, 1.43e-02],\n",
      "        [2.85e-04, 8.03e-05, 5.11e-05,  ..., 3.15e-02, 7.69e-03, 7.16e-01]]), '_normalized': False, '_suppr_inf': False}], [{'_matrice': tensor([[6.64e-02, 6.77e-04, 5.91e-04,  ..., 9.61e-05, 4.01e-02, 7.26e-02],\n",
      "        [2.62e-02, 3.24e-02, 3.64e-03,  ..., 5.80e-04, 6.10e-02, 9.86e-02],\n",
      "        [1.07e-02, 7.32e-03, 2.12e-02,  ..., 6.63e-04, 6.64e-02, 9.57e-02],\n",
      "        ...,\n",
      "        [1.49e-04, 7.72e-05, 6.34e-05,  ..., 1.01e-02, 8.10e-02, 1.49e-01],\n",
      "        [2.38e-03, 3.26e-03, 3.78e-03,  ..., 2.69e-02, 1.30e-02, 1.57e-02],\n",
      "        [1.86e-03, 6.99e-04, 9.92e-04,  ..., 1.65e-03, 8.49e-02, 1.89e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[1.51e-01, 2.42e-02, 1.94e-02,  ..., 7.18e-03, 2.00e-02, 2.80e-02],\n",
      "        [2.59e-01, 1.95e-01, 3.03e-02,  ..., 2.90e-03, 1.12e-02, 9.66e-03],\n",
      "        [8.89e-02, 1.14e-01, 2.44e-01,  ..., 9.22e-04, 9.56e-03, 4.96e-03],\n",
      "        ...,\n",
      "        [2.55e-04, 9.79e-05, 8.21e-05,  ..., 3.61e-01, 2.62e-02, 1.02e-01],\n",
      "        [1.55e-03, 1.23e-03, 1.12e-03,  ..., 9.11e-03, 4.59e-02, 6.47e-02],\n",
      "        [1.71e-03, 7.59e-04, 9.04e-04,  ..., 3.09e-02, 3.77e-02, 1.71e-01]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[5.32e-02, 2.94e-02, 6.56e-03,  ..., 4.28e-04, 3.78e-02, 1.57e-02],\n",
      "        [6.84e-02, 4.75e-02, 3.16e-02,  ..., 2.94e-04, 1.88e-02, 6.58e-03],\n",
      "        [3.84e-02, 2.45e-02, 1.38e-02,  ..., 8.04e-05, 1.00e-02, 1.61e-03],\n",
      "        ...,\n",
      "        [4.37e-04, 1.64e-04, 6.88e-05,  ..., 4.97e-02, 8.69e-02, 7.34e-02],\n",
      "        [1.91e-03, 1.96e-03, 1.01e-03,  ..., 2.65e-02, 3.94e-02, 7.28e-02],\n",
      "        [1.41e-03, 4.51e-04, 2.11e-04,  ..., 2.06e-02, 8.04e-02, 4.57e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.86e-03, 1.58e-02, 1.80e-02,  ..., 1.79e-04, 8.21e-02, 8.07e-03],\n",
      "        [5.23e-03, 1.63e-03, 2.34e-02,  ..., 1.07e-04, 2.01e-02, 1.55e-03],\n",
      "        [2.20e-03, 3.82e-04, 3.26e-03,  ..., 3.12e-05, 4.34e-03, 2.59e-04],\n",
      "        ...,\n",
      "        [6.20e-05, 1.01e-05, 4.85e-06,  ..., 8.67e-03, 1.18e-01, 1.44e-02],\n",
      "        [2.61e-04, 1.36e-04, 9.54e-05,  ..., 9.98e-04, 1.12e-01, 4.93e-03],\n",
      "        [1.67e-04, 1.31e-04, 7.16e-05,  ..., 1.21e-02, 1.11e-01, 5.53e-03]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[2.00e-03, 5.05e-03, 5.97e-03,  ..., 1.26e-04, 1.04e-01, 6.74e-04],\n",
      "        [1.89e-02, 1.38e-03, 5.18e-03,  ..., 1.01e-04, 9.70e-02, 9.88e-04],\n",
      "        [1.81e-02, 2.04e-02, 1.62e-02,  ..., 7.71e-05, 7.12e-02, 8.97e-04],\n",
      "        ...,\n",
      "        [1.82e-04, 1.53e-04, 1.78e-04,  ..., 2.37e-02, 6.37e-02, 1.06e-01],\n",
      "        [4.90e-03, 5.49e-03, 6.21e-03,  ..., 4.31e-03, 4.38e-02, 5.97e-03],\n",
      "        [4.80e-04, 4.29e-04, 2.70e-04,  ..., 6.73e-03, 1.03e-01, 1.23e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[3.11e-04, 7.15e-04, 1.17e-03,  ..., 1.44e-04, 1.09e-01, 1.76e-03],\n",
      "        [6.93e-03, 4.46e-03, 5.09e-02,  ..., 1.51e-04, 8.19e-02, 1.50e-03],\n",
      "        [2.59e-03, 8.25e-03, 1.27e-02,  ..., 4.29e-05, 1.06e-01, 2.43e-03],\n",
      "        ...,\n",
      "        [3.16e-04, 1.19e-04, 9.44e-05,  ..., 4.56e-03, 9.90e-02, 7.47e-02],\n",
      "        [3.39e-03, 3.76e-03, 2.52e-03,  ..., 1.62e-02, 1.80e-02, 3.26e-02],\n",
      "        [1.23e-03, 5.46e-04, 2.46e-04,  ..., 1.27e-02, 8.58e-02, 3.36e-02]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[0.01, 0.01, 0.00,  ..., 0.00, 0.07, 0.01],\n",
      "        [0.03, 0.00, 0.01,  ..., 0.00, 0.07, 0.01],\n",
      "        [0.03, 0.01, 0.01,  ..., 0.00, 0.07, 0.00],\n",
      "        ...,\n",
      "        [0.00, 0.00, 0.00,  ..., 0.01, 0.10, 0.03],\n",
      "        [0.01, 0.00, 0.00,  ..., 0.02, 0.01, 0.15],\n",
      "        [0.00, 0.00, 0.00,  ..., 0.01, 0.05, 0.24]]), '_normalized': False, '_suppr_inf': False}, {'_matrice': tensor([[4.43e-01, 8.17e-02, 1.51e-01,  ..., 1.86e-06, 5.52e-06, 7.59e-04],\n",
      "        [9.33e-01, 2.30e-02, 1.54e-02,  ..., 6.31e-08, 1.58e-07, 2.84e-06],\n",
      "        [2.21e-02, 9.41e-01, 2.42e-02,  ..., 1.42e-08, 6.47e-08, 9.21e-07],\n",
      "        ...,\n",
      "        [1.13e-08, 7.74e-08, 1.94e-08,  ..., 2.53e-02, 2.83e-05, 8.98e-05],\n",
      "        [5.57e-05, 1.57e-04, 1.73e-04,  ..., 1.72e-02, 6.13e-02, 3.13e-03],\n",
      "        [4.97e-06, 3.38e-06, 1.19e-06,  ..., 2.52e-02, 1.07e-01, 2.03e-03]]), '_normalized': False, '_suppr_inf': False}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Snt' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers)):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers[layer])):\n\u001b[0;32m---> 54\u001b[0m         full_matrice_cutted \u001b[38;5;241m=\u001b[39m \u001b[43mUtils_concat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut_matrix_into_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# L x nb_heads x N x N\u001b[39;00m\n",
      "File \u001b[0;32m~/Bureau/Attention3/Utils_concat.py:81\u001b[0m, in \u001b[0;36mcut_matrix_into_sentences\u001b[0;34m(_matrice, snts)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcut_matrix_into_sentences\u001b[39m(_matrice: Matrice, snts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[Matrice]]:\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Découpe une Matrice ctx+crt de self-attention en différentes Matrices k3 x k3, k3 x k2, ..., k0 x k0\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m        list: liste de liste de Matrice\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _matrice\u001b[38;5;241m.\u001b[39mmatrice\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(snt) \u001b[38;5;28;01mfor\u001b[39;00m snt \u001b[38;5;129;01min\u001b[39;00m snts]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG]Size error, matrice size dim0 vs. snt len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_matrice\u001b[38;5;241m.\u001b[39mmatrice\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(snt)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39msnt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39msnts])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _matrice\u001b[38;5;241m.\u001b[39mmatrice\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(snt) \u001b[38;5;28;01mfor\u001b[39;00m snt \u001b[38;5;129;01min\u001b[39;00m snts]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG]Size error, matrice size dim1 vs. snt len: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_matrice\u001b[38;5;241m.\u001b[39mmatrice\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(snt)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39msnt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39msnts])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     debut_row, fin_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Snt' object is not iterable"
     ]
    }
   ],
   "source": [
    "import Concat_matrice\n",
    "from Matrice import Matrice\n",
    "import doctest\n",
    "from typing import List\n",
    "from Snt import Snt\n",
    "import torch\n",
    "import Utils_data\n",
    "import Utils_concat\n",
    "import importlib\n",
    "importlib.reload(Concat_matrice)\n",
    "importlib.reload(Utils_data)\n",
    "importlib.reload(Utils_concat)\n",
    "doctest.testmod()\n",
    "torch.set_printoptions(precision=2)\n",
    "print(f\"[DEBUG] Doctest clear\")\n",
    "_DEBUG_START = True\n",
    "_DEBUG_SUPPR_PAD = True\n",
    "_DEBUG_NORM_TENSOR = True\n",
    "_DEBUG_FUSION_BPE= False\n",
    "_PRECISION = 3\n",
    "_OUTPUT_PATH=f\"/home/getalp/lopezfab/Documents\"\n",
    "id = 1850\n",
    "r_path=f\"/home/getalp/lopezfab/lig/temp/temp/test_attn/{id}.json\"\n",
    "data=Utils_data.lecture_data(r_path)\n",
    "print(data.keys())\n",
    "src_seg_lab = data['src_segments_labels']\n",
    "src = Snt(identifiant=-1, tokens=Utils_concat.ajoute_eos_tokens_src(_snt=data[\"src_tokens\"].split(), src_segments_labels=src_seg_lab))\n",
    "# src = Utils_concat.ajoute_eos_tokens_src(_snt=data[\"src_tokens\"].split(), src_segments_labels=src_seg_lab)\n",
    "id = data['id']\n",
    "\n",
    "# Découpage de la phrase entière en phrase courante + phrases de contexte\n",
    "src_cutted = Utils_concat.full_sentence_to_ctx_and_crt(src)\n",
    "ctxs = []\n",
    "print(f\"[DEBUG] len(full_sentence): {len(src)}\")\n",
    "# \n",
    "for k in range(len(src_cutted[:-1])):\n",
    "    ctxs.append(Snt(identifiant=int(data[\"id\"]) - len(src_cutted[k:-1]), tokens = src_cutted[k]))\n",
    "crt = Snt(identifiant=data['id'], tokens= src_cutted[-1])\n",
    "\n",
    "# Extraction des différentes matrices à travers les 6 layers et les 8 têtes d'attention de chaque layer\n",
    "layers = []\n",
    "for layer in range(len(data['heads_enc_attn'])):\n",
    "    heads = []\n",
    "    for head in range(len(data['heads_enc_attn'][layer])):\n",
    "        full_matrice = torch.tensor(data['heads_enc_attn'][layer][head])\n",
    "        full_matrice = full_matrice.squeeze()\n",
    "        heads.append(Matrice(full_matrice))\n",
    "    layers.append(heads)\n",
    "\n",
    "# Pour chaque layer, pour chaque tête d'attention, on découpe la matrice en combinaison de k3*k3, k3*k2... k2*k3, k2*k2,... crt*crt\n",
    "print(layers)\n",
    "for layer in range(len(layers)):\n",
    "    for head in range(len(layers[layer])):\n",
    "        full_matrice_cutted = Utils_concat.cut_matrix_into_sentences(layers[layer][head], src)[-1]\n",
    "\n",
    "# L x nb_heads x N x N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : 1850\n",
      "dict_keys(['id', 'crt', 'ctxs', 'matrices', 'SL_matrice', 'heads'])\n",
      "Snt(id=1850, tokens=['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<eos>'])\n",
      "Snt(id=1849, tokens=['<pad>', 'When', 'women', 'fle@@', 'e', ',', 'other', 'reasons', 'are', 'in', 'the', 'fore@@', 'ground', ':', '&quot;', 'Many', 'women', 's@@', 'ne@@', 'ak', 'away', 'secre@@', 'tly', ',', 'because', 'they', 'see', 'no', 'other', 'way', 'out', '.', '&quot;', '<eos>'])\n",
      "Snt(id=1848, tokens=['<pad>', '&quot;', 'Men', 'fle@@', 'e', 'because', 'of', 'war', ',', 'because', 'they', 'are', 'politically', 'persec@@', 'uted', ',', 'because', 'they', 'are', 'threatened', 'with', 'torture', 'or', 'death', ',', 'or', 'because', 'their', 'families', 'pin', 'their', 'hopes', 'on', 'them', 'and', 'send', 'them', 'to', 'Europe', ',', '&quot;', 'she', 'explains', '.', '<eos>'])\n",
      "Snt(id=1847, tokens=['<pad>', '&quot;', 'I', '&apos;m', 'not', 'exag@@', 'ger@@', 'ating', 'when', 'I', 'say', 'that', 'every', 'woman', 'who', 'arri@@', 'ves', 'here', 'has', 'dealt', 'with', 'sexual', 'violence', 'on', 'her', 'way', 'to', 'find', 'refu@@', 'ge', ',', '&quot;', 'said', 'B@@', 'ahr', '.', '<eos>'])\n"
     ]
    }
   ],
   "source": [
    "import Utils_data as ud\n",
    "import Utils\n",
    "import torch\n",
    "import Matrice\n",
    "import Sl_matrice\n",
    "from Snt import Snt\n",
    "import importlib\n",
    "for id in [1850]:\n",
    "    print(f\"sentence : {id}\")\n",
    "    precision = 8\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/temp/han_attn2/{id}.json\"\n",
    "    OUTPUT_PATH = f\"/home/getalp/lopezfab/Documents/{id}\"\n",
    "\n",
    "    data=ud.lecture_data(r_path)\n",
    "    crt, ctxs, ctxs_heads, sl_heads = ud.lecture_multi_enc_objet(data)\n",
    "    mask = torch.ones(sl_heads[0].matrice.shape[1], dtype = torch.bool)\n",
    "    # Correction dans les cas où il y a moins de 3 contextes\n",
    "    for k in range(len(ctxs)-1, -1, -1):\n",
    "        # On supprime les contextes inutiles\n",
    "        if len(ctxs[k].tokens) == 1 or (len(ctxs[k].tokens) > 1 and ctxs[k].tokens[-2] == \"<pad>\"):\n",
    "            del ctxs[k]\n",
    "            del ctxs_heads[k]\n",
    "            mask[k] = False\n",
    "        else:\n",
    "            # On corrige un problème de padding qui apparait quand il y a moins de 3 contextes\n",
    "            for h in range(len(ctxs_heads[0])):\n",
    "                ctxs_heads[k][h].matrice = ctxs_heads[k][h].matrice[..., -len(ctxs[k]):]\n",
    "                # print(f\"[DEBUG]ctxs_heads[k][h].size: {ctxs_heads[k][h].matrice.size()}\")\n",
    "\n",
    "    # S'il y a au moins une phrase de contexte\n",
    "    if len(ctxs) >= 1:\n",
    "        for h in range(len(sl_heads)):\n",
    "            sl_heads[h].matrice = sl_heads[h].matrice[:, mask]\n",
    "\n",
    "        # print(f\"Traitement de la phrase {crt.identifiant}\")\n",
    "        print(crt)\n",
    "        # print(ctxs[0])\n",
    "        # print(ctxs[1])\n",
    "        \n",
    "        # print(ctxs[2])\n",
    "        # print(f\"len crt : {len(crt)}\")\n",
    "        # print(f\"len(ctxs_heads: {len(ctxs_heads)}\")\n",
    "        # print(f\"len(ctxs): {[len(ctx) for ctx in ctxs]}\")\n",
    "        # print(f\"len(ctxs_heads[0]): {len(ctxs_heads[0])}\")\n",
    "        # print(f\"ctxs_heads[0][0].size(): {ctxs_heads[0][0].matrice.size()}\")\n",
    "        # print(f\"ctxs_heads[1][0].size(): {ctxs_heads[1][0].matrice.size()}\")\n",
    "        \n",
    "        # Nettoyage de la phrase courante\n",
    "        list_crt_suppr_pad = crt.suppr_pad()\n",
    "        list_crt_fusion_bpe = crt.fusion_bpe()\n",
    "        # print(f\"list_crt_suppr_pad: {list_crt_suppr_pad}\")\n",
    "        # print(f\"list_crt_fusion_bpe: {list_crt_fusion_bpe}\")\n",
    "        # print(sl_heads[0].matrice.size())\n",
    "\n",
    "        mean_ctxs_heads = []\n",
    "        # Traitement de chaque contexte\n",
    "        for k in range(len(ctxs)):\n",
    "            # Nettoyage de la phrases de contexte K\n",
    "            list_ctx_suppr_pad = ctxs[k].suppr_pad()\n",
    "            # print(f\"len(ctxs[k]) : {len(ctxs[k])}\")\n",
    "            list_ctx_fusion_bpe = ctxs[k].fusion_bpe()\n",
    "            # print(list_ctx_fusion_bpe)\n",
    "\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                # Traitement de chaque tête d'attention entre la phrase courante et chaque phrase\n",
    "                ctxs_heads[k][head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad, col_list_suppr_pad=  list_ctx_suppr_pad)\n",
    "                # print(f\"size ctxs_heads[k][h] : {ctxs_heads[k][h].matrice.size()}\")\n",
    "                ctxs_heads[k][head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, col_list_fusion_bpe= list_ctx_fusion_bpe)\n",
    "                ctxs_heads[k][head].suppr_inf()\n",
    "                # ctxs_heads[k][head].norm_tensor()\n",
    "            # Pour chaque phrase de contexte, on récupère la moyenne des poids d'attention de la phrase courante entre la phrase de contexte (doit être effectuée avant la normalisation )\n",
    "            mean_ctxs_heads.append(Matrice.Matrice(Utils.mean_matrices([ctxs_heads[k][head].matrice for head in range(len(ctxs_heads[k])) ])))\n",
    "            mean_ctxs_heads[k].norm_tensor()\n",
    "\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Traitement de chaque tête d'attention entre la phrase courante et l'ensemble des phrases de contexte\n",
    "            sl_heads[sl_head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad)\n",
    "            sl_heads[sl_head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, action= \"mean\")\n",
    "            # sl_heads[sl_head].norm_tensor()\n",
    "            # sl_heads[sl_head].clean_matrice()\n",
    "\n",
    "        # On récupère la moyenne des têtes d'attention du mécanisme sentence level\n",
    "        mean_sl_heads = Sl_matrice.Sl_matrice(Utils.mean_matrices([sl_heads[sl_head].matrice for sl_head in range(len(sl_heads))]))\n",
    "        mean_sl_heads.norm_tensor()\n",
    "\n",
    "        # ToDo: Faire la connection entre le mécanisme d'attention word-level et le mécanisme d'attention sentence-level\n",
    "        full_ctx = ctxs[0] if len(ctxs)>= 1 else None\n",
    "        if full_ctx is not None:\n",
    "            for k in range(1, len(ctxs)):\n",
    "                full_ctx += ctxs[k]\n",
    "\n",
    "\n",
    "        for k in range(len(ctxs)):\n",
    "            # Pour chaque phrase de contexte K, \n",
    "            # On écrit les matrices de chaque heads entre la phrase  courante et la phrase de contexte K\n",
    "            # Puis la moyenne de têtes\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                ctxs_heads[k][head].ecriture_xslx(crt= crt, \n",
    "                                                    ctx= ctxs[k],\n",
    "                                                    absolute_folder= f\"{OUTPUT_PATH}/token_level/{head}\", \n",
    "                                                    filename=f\"ctx_{k}_head_{head}\", \n",
    "                                                    precision=precision, \n",
    "                                                    create_folder_path=True)\n",
    "            mean_ctxs_heads[k].ecriture_xslx(crt= crt,\n",
    "                                                ctx= ctxs[k],\n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/token_level\", \n",
    "                                                filename=f\"mean_ctx_{k}\", \n",
    "                                                create_folder_path=True)\n",
    "\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Pour chaque tête d'attention sentence-level,\n",
    "            # On écrit la tête d'attention entre la phrase courante et les K phrases\n",
    "            # Puis on écrit la moyenne des têtes d'attention\n",
    "            # TODO: à vérifier si sl_heads est en mode k3 x k2 x k1 ou k1 x k2 x k3\n",
    "            # Si sl_heads est en mode  k1 x k2 x k3 alors .flip le passe en mode k3 x k2 x k1 pour plus de lisibilité\n",
    "            sl_heads[sl_head].matrice=sl_heads[sl_head].matrice.flip(1)\n",
    "            sl_heads[sl_head].ecriture_xslx(crt= crt, \n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                                filename=f\"sl_head_{sl_head}\", \n",
    "                                                precision=precision, \n",
    "                                                create_folder_path=True)\n",
    "        mean_sl_heads.ecriture_xslx(crt= crt,\n",
    "                                        absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                        filename=f\"mean\", \n",
    "                                        precision=precision, \n",
    "                                        create_folder_path=True)\n",
    "\n",
    "        # Traitement des phrases contextualisé (phrase crt x [k3, k2, k1])\n",
    "        for h_sl in range(len(sl_heads)):\n",
    "            for h_tl in range(len(ctxs_heads[0])):\n",
    "                \n",
    "                temp = sl_heads[h_sl].contextualise_matrice([ctxs_heads[k][h_tl] for k in range(sl_heads[h_sl].matrice.size(1))])\n",
    "                # temp.suppr_inf()\n",
    "                # temp.norm_tensor()\n",
    "                # print(f\"ctxs: {ctxs}\")\n",
    "                \n",
    "                \n",
    "                temp.ecriture_xslx(crt= crt,\n",
    "                                            ctx= full_ctx,\n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/full_matrice/sl_{h_sl}\", \n",
    "                                            filename=f\"head_{h_tl}\", \n",
    "                                            precision=8,\n",
    "                                            create_folder_path=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'crt', 'ctxs', 'matrices', 'SL_matrice', 'heads'])\n",
      "Traitement de la phrase 1850\n",
      "Snt(id=1849, tokens=['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'When', 'women', 'fle@@', 'e', ',', 'other', 'reasons', 'are', 'in', 'the', 'fore@@', 'ground', ':', '&quot;', 'Many', 'women', 's@@', 'ne@@', 'ak', 'away', 'secre@@', 'tly', ',', 'because', 'they', 'see', 'no', 'other', 'way', 'out', '.', '&quot;', '<eos>'])\n",
      "len(ctxs_heads: 3\n",
      "len(ctxs): [106, 106, 106]\n",
      "len(ctxs_heads[0]): 8\n",
      "ctxs_heads[0][0].size(): torch.Size([106, 106])\n",
      "list_crt_suppr_pad: [80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "list_crt_fusion_bpe: [[20, 19], [17, 16, 15]]\n",
      "torch.Size([106, 3])\n"
     ]
    }
   ],
   "source": [
    "import Utils_data as ud\n",
    "import Utils\n",
    "import torch\n",
    "import Matrice\n",
    "import Sl_matrice\n",
    "import importlib\n",
    "id = 1850\n",
    "precision = 8\n",
    "r_path=f\"/home/getalp/lopezfab/lig/temp/temp/temp/han_attn2/{id}.json\"\n",
    "OUTPUT_PATH = f\"/home/getalp/lopezfab/Documents/{id}\"\n",
    "\n",
    "data=ud.lecture_data(r_path)\n",
    "crt, ctxs, ctxs_heads, sl_heads = ud.lecture_multi_enc_objet(data)\n",
    "\n",
    "print(f\"Traitement de la phrase {crt.identifiant}\")\n",
    "print(ctxs[0])\n",
    "print(f\"len(ctxs_heads: {len(ctxs_heads)}\")\n",
    "print(f\"len(ctxs): {[len(ctx) for ctx in ctxs]}\")\n",
    "print(f\"len(ctxs_heads[0]): {len(ctxs_heads[0])}\")\n",
    "print(f\"ctxs_heads[0][0].size(): {ctxs_heads[0][0].matrice.size()}\")\n",
    "# Nettoyage de la phrase courante\n",
    "list_crt_suppr_pad = crt.suppr_pad()\n",
    "list_crt_fusion_bpe = crt.fusion_bpe()\n",
    "print(f\"list_crt_suppr_pad: {list_crt_suppr_pad}\")\n",
    "print(f\"list_crt_fusion_bpe: {list_crt_fusion_bpe}\")\n",
    "print(sl_heads[0].matrice.size())\n",
    "\n",
    "mean_ctxs_heads = []\n",
    "# Traitement de chaque contexte\n",
    "for k in range(len(ctxs)):\n",
    "    # Nettoyage de la phrases de contexte K\n",
    "    list_ctx_suppr_pad = ctxs[k].suppr_pad()\n",
    "    list_ctx_fusion_bpe = ctxs[k].fusion_bpe()\n",
    "\n",
    "    for head in range(len(ctxs_heads[k])):\n",
    "        # Traitement de chaque tête d'attention entre la phrase courante et chaque phrase\n",
    "        ctxs_heads[k][head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad, col_list_suppr_pad=  list_ctx_suppr_pad)\n",
    "        ctxs_heads[k][head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, col_list_fusion_bpe= list_ctx_fusion_bpe)\n",
    "        # ctxs_heads[k][head].suppr_inf()\n",
    "        # ctxs_heads[k][head].norm_tensor()\n",
    "    # Pour chaque phrase de contexte, on récupère la moyenne des poids d'attention de la phrase courante entre la phrase de contexte (doit être effectuée avant la normalisation )\n",
    "    mean_ctxs_heads.append(Matrice.Matrice(Utils.mean_matrices([ctxs_heads[k][head].matrice for head in range(len(ctxs_heads[k])) ])))\n",
    "    mean_ctxs_heads[k].norm_tensor()\n",
    "\n",
    "for sl_head in range(len(sl_heads)):\n",
    "    # Traitement de chaque tête d'attention entre la phrase courante et l'ensemble des phrases de contexte\n",
    "    sl_heads[sl_head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad)\n",
    "    sl_heads[sl_head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, action= \"mean\")\n",
    "    # sl_heads[sl_head].norm_tensor()\n",
    "    # sl_heads[sl_head].clean_matrice()\n",
    "\n",
    "# On récupère la moyenne des têtes d'attention du mécanisme sentence level\n",
    "mean_sl_heads = Sl_matrice.Sl_matrice(Utils.mean_matrices([sl_heads[sl_head].matrice for sl_head in range(len(sl_heads))]))\n",
    "mean_sl_heads.norm_tensor()\n",
    "\n",
    "# ToDo: Faire la connection entre le mécanisme d'attention word-level et le mécanisme d'attention sentence-level\n",
    "\n",
    "\n",
    "for k in range(len(ctxs)):\n",
    "    # Pour chaque phrase de contexte K, \n",
    "    # On écrit les matrices de chaque heads entre la phrase  courante et la phrase de contexte K\n",
    "    # Puis la moyenne de têtes\n",
    "    for head in range(len(ctxs_heads[k])):\n",
    "        ctxs_heads[k][head].ecriture_xslx(crt= crt, \n",
    "                                            ctx= ctxs[k],\n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/token_level/{head}\", \n",
    "                                            filename=f\"ctx_{k}_head_{head}\", \n",
    "                                            precision=precision, \n",
    "                                            create_folder_path=True)\n",
    "    mean_ctxs_heads[k].ecriture_xslx(crt= crt,\n",
    "                                        ctx= ctxs[k],\n",
    "                                        absolute_folder= f\"{OUTPUT_PATH}/token_level\", \n",
    "                                        filename=f\"mean_ctx_{k}\", \n",
    "                                        create_folder_path=True)\n",
    "\n",
    "for sl_head in range(len(sl_heads)):\n",
    "    # Pour chaque tête d'attention sentence-level,\n",
    "    # On écrit la tête d'attention entre la phrase courante et les K phrases\n",
    "    # Puis on écrit la moyenne des têtes d'attention\n",
    "    # TODO: à vérifier si sl_heads est en mode k3 x k2 x k1 ou k1 x k2 x k3\n",
    "    # Si sl_heads est en mode  k1 x k2 x k3 alors .flip le passe en mode k3 x k2 x k1 pour plus de lisibilité\n",
    "    sl_heads[sl_head].matrice=sl_heads[sl_head].matrice.flip(1)\n",
    "    sl_heads[sl_head].ecriture_xslx(crt= crt, \n",
    "                                        absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                        filename=f\"sl_head_{sl_head}\", \n",
    "                                        precision=precision, \n",
    "                                        create_folder_path=True)\n",
    "mean_sl_heads.ecriture_xslx(crt= crt,\n",
    "                                absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                filename=f\"mean\", \n",
    "                                precision=precision, \n",
    "                                create_folder_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3367, dtype=torch.float64)\n",
      "tensor(0.0896, dtype=torch.float64)\n",
      "tensor(0.0302, dtype=torch.float64)\n",
      "tensor(0.0302, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sl_heads[7].matrice[6, 0])\n",
    "print(ctxs_heads[0][7].matrice[6,14])\n",
    "print(ctxs_heads[0][7].matrice[6,14] * sl_heads[7].matrice[6, 0])\n",
    "print(temp.matrice[6, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 3])\n",
      "torch.Size([23, 29])\n",
      "torch.Size([23, 43])\n",
      "torch.Size([23, 32])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.08220548182725906 vs. t: 0.35071104764938354\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Création du dossier : /home/getalp/lopezfab/Documents/1850/full_matrice\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.05523921549320221 vs. t: 0.3359013497829437\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.07990622520446777 vs. t: 0.363574355840683\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.03890309855341911 vs. t: 0.3090135157108307\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.05947283282876015 vs. t: 0.3598552346229553\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.031393054872751236 vs. t: 0.3371981978416443\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.07453405112028122 vs. t: 0.3488973081111908\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "[DEBUG] Sl.matrice.__mul__() nb row: 23\n",
      "[DEBUG] Sl.matrice.__mul__() nb col: 3\n",
      "test\n",
      "k : 0.08958246558904648 vs. t: 0.3367302119731903\n",
      "[DEBUG] full_matrice size : torch.Size([23, 104])\n",
      "torch.Size([23, 104])\n"
     ]
    }
   ],
   "source": [
    "print(sl_heads[0].matrice.size())\n",
    "print(ctxs_heads[0][1].matrice.size())\n",
    "print(ctxs_heads[1][1].matrice.size())\n",
    "print(ctxs_heads[2][1].matrice.size())\n",
    "for h in range(len(sl_heads)):\n",
    "    temp = sl_heads[h].contextualise_matrice([ctxs_heads[0][h], ctxs_heads[1][h], ctxs_heads[2][h]])\n",
    "    temp.suppr_inf()\n",
    "    temp.norm_tensor()\n",
    "    temp.ecriture_xslx(crt= crt,\n",
    "                                ctx= ctxs[0] + ctxs[1] + ctxs[2],\n",
    "                                absolute_folder= f\"{OUTPUT_PATH}/full_matrice\", \n",
    "                                filename=f\"head_{h}\", \n",
    "                                precision=8,\n",
    "                                create_folder_path=True)\n",
    "print(temp.matrice.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Multienc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils_data as Utils\n",
    "import torch\n",
    "import Matrice\n",
    "import Sl_matrice\n",
    "import importlib\n",
    "for id in range(len(0, 2308)):\n",
    "    print(f\"lecture id : {id}\")\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/temp/han_attn2/{id}.json\"\n",
    "    _DEBUG_TRAITEMENT = False\n",
    "    OUTPUT_PATH = f\"/home/getalp/lopezfab/Documents/{id}\"\n",
    "\n",
    "    data=Utils.lecture_data(r_path)\n",
    "    crt, ctxs, ctxs_heads, sl_heads = Utils.lecture_multi_enc_objet(data)\n",
    "\n",
    "    # print(f\"Traitement de la phrase {crt.identifiant}\")\n",
    "    # print(ctxs[0])\n",
    "    # print(f\"len(ctxs_heads: {len(ctxs_heads)}\")\n",
    "    # print(f\"len(ctxs): {[len(ctx) for ctx in ctxs]}\")\n",
    "    # print(f\"len(ctxs_heads[0]): {len(ctxs_heads[0])}\")\n",
    "    # print(f\"ctxs_heads[0][0].size(): {ctxs_heads[0][0].matrice.size()}\")\n",
    "    print(f\" - suppr pad()\")\n",
    "    list_crt_suppr_pad = crt.suppr_pad()\n",
    "    print(f\" - fusion bpe()\")\n",
    "    list_crt_fusion_bpe = crt.fusion_bpe()\n",
    "    # print(f\"list_crt_suppr_pad: {list_crt_suppr_pad}\")\n",
    "    # print(f\"list_crt_fusion_bpe: {list_crt_fusion_bpe}\")\n",
    "    # print(sl_heads[0].matrice.size())\n",
    "\n",
    "    mean_ctxs_heads = []\n",
    "    for k in range(len(ctxs)):\n",
    "        if _DEBUG_TRAITEMENT:\n",
    "            print(f\"** Traitement du contexte {k}\")\n",
    "        list_ctx_suppr_pad = ctxs[k].suppr_pad()\n",
    "        list_ctx_fusion_bpe = ctxs[k].fusion_bpe()\n",
    "\n",
    "        for head in range(len(ctxs_heads[k])):\n",
    "            if _DEBUG_TRAITEMENT:\n",
    "                print(f\"   traitement de la head: {head}\")\n",
    "            ctxs_heads[k][head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad, col_list_suppr_pad=  list_ctx_suppr_pad)\n",
    "            ctxs_heads[k][head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, col_list_fusion_bpe= list_ctx_fusion_bpe)\n",
    "            ctxs_heads[k][head].clean_matrice()\n",
    "            ctxs_heads[k][head].norm_tensor()\n",
    "        mean_ctxs_heads.append(Matrice.Matrice(Utils.mean_matrices([ctxs_heads[k][head].matrice for head in range(len(ctxs_heads[k])) ])))\n",
    "        mean_ctxs_heads[k].norm_tensor()\n",
    "\n",
    "    for sl_head in range(len(sl_heads)):\n",
    "        if _DEBUG_TRAITEMENT:\n",
    "            print(f\"   traitement de la sl_head: {sl_head}\")\n",
    "        sl_heads[sl_head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad)\n",
    "        sl_heads[sl_head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, action= torch.mean)\n",
    "        sl_heads[sl_head].norm_tensor()\n",
    "        # sl_heads[sl_head].clean_matrice()\n",
    "    mean_sl_heads = Sl_matrice.Sl_matrice(Utils.mean_matrices([sl_heads[sl_head].matrice for sl_head in range(len(sl_heads))]))\n",
    "    mean_sl_heads.norm_tensor()\n",
    "\n",
    "\n",
    "    for k in range(len(ctxs)):\n",
    "        # print(f\"écriture du contexte {k}\")\n",
    "        for head in range(len(ctxs_heads[k])):\n",
    "            # print(f\"écriture de la head {head}\")\n",
    "\n",
    "            ctxs_heads[k][head].ecriture_xslx(crt= crt, \n",
    "                                                ctx= ctxs[k],\n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/token_level/{head}\", \n",
    "                                                filename=f\"ctx_{k}_head_{head}\", \n",
    "                                                create_folder_path=True)\n",
    "        mean_ctxs_heads[k].ecriture_xslx(crt= crt,\n",
    "                                            ctx= ctxs[k],\n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/token_level\", \n",
    "                                            filename=f\"mean_ctx_{k}\", \n",
    "                                            create_folder_path=True)\n",
    "    for sl_head in range(len(sl_heads)):\n",
    "        # print(f\"écriture de la sl_head {sl_head}\")\n",
    "        sl_heads[sl_head].ecriture_xslx(crt= crt, \n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/sentence_level/{sl_head}\", \n",
    "                                            filename=f\"sl_head{sl_head}\", \n",
    "                                            create_folder_path=True)\n",
    "    mean_sl_heads.ecriture_xslx(crt= crt,\n",
    "                                    absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                    filename=f\"mean_sl_head\", \n",
    "                                    create_folder_path=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
