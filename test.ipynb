{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Concat√©nation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Production d'une Snt de Test\n",
      "[DEBUG] Production d'une Matrice de Test\n",
      "[DEBUG] Production d'une Sl_matrice de Test\n",
      "[DEBUG] Production d'une Matrice de Test\n",
      "[DEBUG] Production d'une Matrice de Test\n",
      "{'_crt': {'_identifiant': 42, '_tokens': ['0', 'Pro', 'Duc', 'tion', \"d'\", 'une', 'Snt', 'de', 'test', '.']}, '_ctxs': [{'_identifiant': 42, '_tokens': ['0', 'Pro', 'Duc', 'tion', \"d'\", 'une', 'Snt', 'de', 'test', '.']}, {'_identifiant': 42, '_tokens': ['0', 'Pro', 'Duc', 'tion', \"d'\", 'une', 'Snt', 'de', 'test', '.']}, {'_identifiant': 42, '_tokens': ['0', 'Pro', 'Duc', 'tion', \"d'\", 'une', 'Snt', 'de', 'test', '.']}], '_heads': [{'_matrice': tensor([[9., 0., 1., 5., 5., 7., 3., 1., 3., 2.],\n",
      "        [6., 2., 5., 0., 8., 0., 8., 7., 7., 0.],\n",
      "        [3., 6., 3., 2., 4., 0., 6., 4., 0., 1.],\n",
      "        [3., 6., 7., 2., 4., 3., 5., 6., 6., 4.],\n",
      "        [6., 8., 3., 6., 8., 6., 5., 2., 6., 8.],\n",
      "        [9., 2., 6., 7., 2., 3., 6., 1., 8., 1.],\n",
      "        [2., 9., 1., 0., 9., 0., 2., 1., 9., 9.],\n",
      "        [3., 8., 3., 1., 1., 9., 2., 4., 8., 3.],\n",
      "        [1., 1., 4., 9., 1., 2., 1., 5., 3., 2.],\n",
      "        [3., 2., 5., 1., 7., 2., 5., 6., 1., 6.]])}, {'_matrice': tensor([[9., 0., 1., 5., 5., 7., 3., 1., 3., 2.],\n",
      "        [6., 2., 5., 0., 8., 0., 8., 7., 7., 0.],\n",
      "        [3., 6., 3., 2., 4., 0., 6., 4., 0., 1.],\n",
      "        [3., 6., 7., 2., 4., 3., 5., 6., 6., 4.],\n",
      "        [6., 8., 3., 6., 8., 6., 5., 2., 6., 8.],\n",
      "        [9., 2., 6., 7., 2., 3., 6., 1., 8., 1.],\n",
      "        [2., 9., 1., 0., 9., 0., 2., 1., 9., 9.],\n",
      "        [3., 8., 3., 1., 1., 9., 2., 4., 8., 3.],\n",
      "        [1., 1., 4., 9., 1., 2., 1., 5., 3., 2.],\n",
      "        [3., 2., 5., 1., 7., 2., 5., 6., 1., 6.]])}, {'_matrice': tensor([[9., 0., 1., 5., 5., 7., 3., 1., 3., 2.],\n",
      "        [6., 2., 5., 0., 8., 0., 8., 7., 7., 0.],\n",
      "        [3., 6., 3., 2., 4., 0., 6., 4., 0., 1.],\n",
      "        [3., 6., 7., 2., 4., 3., 5., 6., 6., 4.],\n",
      "        [6., 8., 3., 6., 8., 6., 5., 2., 6., 8.],\n",
      "        [9., 2., 6., 7., 2., 3., 6., 1., 8., 1.],\n",
      "        [2., 9., 1., 0., 9., 0., 2., 1., 9., 9.],\n",
      "        [3., 8., 3., 1., 1., 9., 2., 4., 8., 3.],\n",
      "        [1., 1., 4., 9., 1., 2., 1., 5., 3., 2.],\n",
      "        [3., 2., 5., 1., 7., 2., 5., 6., 1., 6.]])}], '_sl_heads': [{'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}, {'_matrice': tensor([[9., 3., 2.],\n",
      "        [6., 4., 3.],\n",
      "        [7., 3., 3.],\n",
      "        [1., 8., 9.],\n",
      "        [2., 2., 4.],\n",
      "        [4., 5., 4.],\n",
      "        [4., 2., 2.],\n",
      "        [2., 9., 8.],\n",
      "        [0., 5., 5.],\n",
      "        [2., 8., 3.]])}], '_ctxs_heads': [[{'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}], [{'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}], [{'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}, {'_matrice': tensor([[5., 1., 5., 6., 7., 5., 8., 5., 2., 2.],\n",
      "        [7., 1., 6., 0., 8., 9., 6., 2., 6., 9.],\n",
      "        [6., 0., 9., 4., 2., 8., 0., 7., 7., 9.],\n",
      "        [5., 6., 7., 3., 3., 3., 0., 3., 9., 4.],\n",
      "        [7., 1., 4., 7., 3., 3., 7., 8., 7., 5.],\n",
      "        [3., 0., 3., 0., 1., 3., 2., 1., 6., 6.],\n",
      "        [3., 8., 1., 1., 0., 5., 9., 2., 4., 7.],\n",
      "        [8., 7., 8., 9., 2., 5., 1., 5., 8., 3.],\n",
      "        [5., 5., 6., 6., 4., 4., 7., 5., 6., 1.],\n",
      "        [4., 6., 2., 7., 5., 5., 1., 6., 8., 8.]])}]]}\n"
     ]
    }
   ],
   "source": [
    "from Classes.Multi_enc_matrice import Multi_enc_matrice\n",
    "test = Multi_enc_matrice()\n",
    "test.test_multi_enc_matrice()\n",
    "\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] id: 188\n",
      "[['<eos>'], ['So', 'over', 'the', 'long', 'course', 'of', 'human', 'history', ',', 'the', 'infec@@', 'tious', 'disease', 'that', '&apos;s', 'killed', 'more', 'humans', 'than', 'any', 'other', 'is', 'malaria', '.', '<eos>'], ['It', '&apos;s', 'carried', 'in', 'the', 'bit@@', 'es', 'of', 'infected', 'mo@@', 'squ@@', 'it@@', 'os', ',', 'and', 'it', '&apos;s', 'probably', 'our', 'oldest', 's@@', 'cour@@', 'ge', '.', '<END>']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Classes.Snt import Snt\n",
    "from Classes.Matrice import Matrice\n",
    "import Utils_data\n",
    "import Utils_concat\n",
    "import importlib\n",
    "importlib.reload(Utils_data)\n",
    "importlib.reload(Utils_concat)\n",
    "_FULL_SNT = False # permet prendre en compte la phrase courante ou non\n",
    "id = 1850\n",
    "\n",
    "for id in range(188, 189):\n",
    "    print(f\"[DEBUG] id: {id}\")\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/test_attn/{id}.json\"\n",
    "    data=Utils_data.lecture_data(r_path)\n",
    "    _OUTPUT_PATH=f\"/home/getalp/lopezfab/Documents/concat/{id}\"\n",
    "\n",
    "    # Traitement des Phrases extraites\n",
    "    ssl = data['src_segments_labels']\n",
    "    src = Snt(identifiant=-1, tokens=Utils_concat.ajoute_eos_tokens_src(_snt=data[\"src_tokens\"].split(), src_segments_labels=ssl))\n",
    "    src_cutted = Utils_concat.full_sentence_to_ctx_and_crt(src)\n",
    "\n",
    "    # Au moins une phrase de contexte et la phrase courante (+ 1 car contient une phrase vide quand le nb de contexte est inf√©rieur √† la normale)\n",
    "    if len(src_cutted) > 2: \n",
    "        # Extraction des phrases de contexte et de la phrase courante\n",
    "        ctxs = []\n",
    "        for k in range(len(src_cutted[:-1])):\n",
    "            ctxs.append(Snt(identifiant=int(data[\"id\"]) - len(src_cutted[k:-1]), tokens = src_cutted[k]))\n",
    "        # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "\n",
    "        # Extraction des diff√©rentes matrices √† travers les 6 layers et les 8 t√™tes d'attention de chaque layer\n",
    "        layers = []\n",
    "        for layer in range(len(data['heads_enc_attn'])): # Pour chaque layer\n",
    "            heads = []\n",
    "            for head in range(len(data['heads_enc_attn'][layer])): # on extrait chaque t√™te par layer\n",
    "                full_matrice = torch.tensor(data['heads_enc_attn'][layer][head])\n",
    "                full_matrice = full_matrice.squeeze() # on supprime une dimension qui semble inutile (=1)\n",
    "                heads.append(Matrice(full_matrice))\n",
    "            layers.append(heads)\n",
    "        # layers : L x [nb_heads x [torch.Tensor(N x N)]]\n",
    "\n",
    "        # Traitement du cas particulier o√π un contexte n'est pas pr√©sent. Suppression des reliquats dans le contexte et les matrices d'attention\n",
    "        for i in range(len(ctxs)-1, -1, -1):\n",
    "            if ctxs[i].tokens == [\"<eos>\"]:\n",
    "                del ctxs[i]\n",
    "                del src_cutted[i]\n",
    "                for layer in range(len(layers)):\n",
    "                    for head in range(len(layers[layer])):\n",
    "                        layers[layer][head].matrice = torch.cat([layers[layer][head].matrice[1:, 1:]])\n",
    "\n",
    "        # R√©cup√©ration de l'ensemble des tokens\n",
    "        full_ctx = Snt(identifiant= id - len(ctxs), tokens= ctxs[0].tokens)\n",
    "        if len(ctxs) > 1:\n",
    "            for snt in ctxs[1:]:\n",
    "                full_ctx.tokens += snt.tokens\n",
    "                # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "        crt = Snt(identifiant=data['id'], tokens= src_cutted[-1])\n",
    "        if _FULL_SNT: # permet prendre en compte la phrase courante ou non\n",
    "            full_ctx.tokens += crt.tokens\n",
    "\n",
    "        # print((f\"[DEBUG] taille des matrices: {layers[0][0].matrice.size()}\"))\n",
    "        # print(f\"[DEBUG] taille des phrases compl√®te vs tailles respectives: {sum([len(ctx) for ctx in ctxs] + [len(crt)])} vs. [{[len(ctx) for ctx in ctxs]}, {len(crt)}]\")\n",
    "\n",
    "        # Pour chaque layer, pour chaque t√™te d'attention, on d√©coupe la matrice en combinaison de k3*k3, k3*k2... k2*k3, k2*k2,... crt*crt\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                full_matrice_cutted = Utils_concat.cut_matrix_into_sentences(layers[layer][head], src_cutted)\n",
    "                # Derni√®re liste correspond √† la phrase courante vers les phrases de contexte et la phrase courante\n",
    "                # _FULL_SNT permet prendre en compte la phrase courante ou non\n",
    "                if _FULL_SNT:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:]], dim = 1)\n",
    "                else:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:-1]], dim = 1)\n",
    "        # print(f\"[DEBUG] taille de la matrice d√©coup√©e: {[matrice.matrice.size() for matrice in full_matrice_cutted[-1]]}\")\n",
    "        # print(f\"[DEBUG] taille de la matrice reconstitu√©e: {layers[layer][head].matrice.size()}\")\n",
    "        # print(f\"[DEBUG] √©l√©ment matrice: {layers[0][0].matrice[-2, ...]}\")\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                layers[layer][head].suppr_inf()\n",
    "                layers[layer][head].norm_tensor()\n",
    "                layers[layer][head].ecriture_xslx(crt=crt, ctx=full_ctx, precision= 4, absolute_folder=f\"{_OUTPUT_PATH}/full_matrice/{layer}\", filename=f\"{head}\", create_folder_path=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] id: 1850\n",
      "[DEBUG] data.keys(): dict_keys(['id', 'src_tokens', 'src_segments_labels', 'tgt_tokens', 'tgt_eos_pos', 'enc_attn', 'dec_attn', 'heads_enc_attn', 'heads_dec_attn'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Classes.Snt import Snt\n",
    "from Classes.Matrice import Matrice\n",
    "import Utils_data\n",
    "import Utils_concat\n",
    "import importlib\n",
    "importlib.reload(Utils_data)\n",
    "importlib.reload(Utils_concat)\n",
    "_FULL_SNT = False # permet prendre en compte la phrase courante ou non\n",
    "id = 1850\n",
    "\n",
    "print(f\"[DEBUG] id: {id}\")\n",
    "r_path=f\"/home/getalp/lopezfab/lig/temp/temp/test_attn/{id}.json\"\n",
    "_OUTPUT_PATH=f\"/home/getalp/lopezfab/Documents/concat/{id}\"\n",
    "data=Utils_data.lecture_data(r_path)\n",
    "print(f\"[DEBUG] data.keys(): {data.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Classes.Snt import Snt\n",
    "from Classes.Matrice import Matrice\n",
    "import Utils_data\n",
    "import Utils_concat\n",
    "import importlib\n",
    "importlib.reload(Utils_data)\n",
    "importlib.reload(Utils_concat)\n",
    "_FULL_SNT = False # permet prendre en compte la phrase courante ou non\n",
    "id = 1850\n",
    "\n",
    "for id in range(188, 189):\n",
    "    print(f\"[DEBUG] id: {id}\")\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/test_attn/{id}.json\"\n",
    "    data=Utils_data.lecture_data(r_path)\n",
    "    _OUTPUT_PATH=f\"/home/getalp/lopezfab/Documents/concat/{id}\"\n",
    "\n",
    "    # Traitement des Phrases extraites\n",
    "    ssl = data['src_segments_labels']\n",
    "    src = Snt(identifiant=-1, tokens=Utils_concat.ajoute_eos_tokens_src(_snt=data[\"src_tokens\"].split(), src_segments_labels=ssl))\n",
    "    src_cutted = Utils_concat.full_sentence_to_ctx_and_crt(src)\n",
    "\n",
    "    # Au moins une phrase de contexte et la phrase courante (+ 1 car contient une phrase vide quand le nb de contexte est inf√©rieur √† la normale)\n",
    "    if len(src_cutted) > 2: \n",
    "        # Extraction des phrases de contexte et de la phrase courante\n",
    "        ctxs = []\n",
    "        for k in range(len(src_cutted[:-1])):\n",
    "            ctxs.append(Snt(identifiant=int(data[\"id\"]) - len(src_cutted[k:-1]), tokens = src_cutted[k]))\n",
    "        # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "\n",
    "        # Extraction des diff√©rentes matrices √† travers les 6 layers et les 8 t√™tes d'attention de chaque layer\n",
    "        layers = []\n",
    "        for layer in range(len(data['heads_enc_attn'])): # Pour chaque layer\n",
    "            heads = []\n",
    "            for head in range(len(data['heads_enc_attn'][layer])): # on extrait chaque t√™te par layer\n",
    "                full_matrice = torch.tensor(data['heads_enc_attn'][layer][head])\n",
    "                full_matrice = full_matrice.squeeze() # on supprime une dimension qui semble inutile (=1)\n",
    "                heads.append(Matrice(full_matrice))\n",
    "            layers.append(heads)\n",
    "        # layers : L x [nb_heads x [torch.Tensor(N x N)]]\n",
    "\n",
    "        # Traitement du cas particulier o√π un contexte n'est pas pr√©sent. Suppression des reliquats dans le contexte et les matrices d'attention\n",
    "        for i in range(len(ctxs)-1, -1, -1):\n",
    "            if ctxs[i].tokens == [\"<eos>\"]:\n",
    "                del ctxs[i]\n",
    "                del src_cutted[i]\n",
    "                for layer in range(len(layers)):\n",
    "                    for head in range(len(layers[layer])):\n",
    "                        layers[layer][head].matrice = torch.cat([layers[layer][head].matrice[1:, 1:]])\n",
    "\n",
    "        # R√©cup√©ration de l'ensemble des tokens\n",
    "        full_ctx = Snt(identifiant= id - len(ctxs), tokens= ctxs[0].tokens)\n",
    "        if len(ctxs) > 1:\n",
    "            for snt in ctxs[1:]:\n",
    "                full_ctx.tokens += snt.tokens\n",
    "                # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "        crt = Snt(identifiant=data['id'], tokens= src_cutted[-1])\n",
    "        if _FULL_SNT: # permet prendre en compte la phrase courante ou non\n",
    "            full_ctx.tokens += crt.tokens\n",
    "\n",
    "        # print((f\"[DEBUG] taille des matrices: {layers[0][0].matrice.size()}\"))\n",
    "        # print(f\"[DEBUG] taille des phrases compl√®te vs tailles respectives: {sum([len(ctx) for ctx in ctxs] + [len(crt)])} vs. [{[len(ctx) for ctx in ctxs]}, {len(crt)}]\")\n",
    "\n",
    "        # Pour chaque layer, pour chaque t√™te d'attention, on d√©coupe la matrice en combinaison de k3*k3, k3*k2... k2*k3, k2*k2,... crt*crt\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                full_matrice_cutted = Utils_concat.cut_matrix_into_sentences(layers[layer][head], src_cutted)\n",
    "                # Derni√®re liste correspond √† la phrase courante vers les phrases de contexte et la phrase courante\n",
    "                # _FULL_SNT permet prendre en compte la phrase courante ou non\n",
    "                if _FULL_SNT:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:]], dim = 1)\n",
    "                else:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:-1]], dim = 1)\n",
    "        # print(f\"[DEBUG] taille de la matrice d√©coup√©e: {[matrice.matrice.size() for matrice in full_matrice_cutted[-1]]}\")\n",
    "        # print(f\"[DEBUG] taille de la matrice reconstitu√©e: {layers[layer][head].matrice.size()}\")\n",
    "        # print(f\"[DEBUG] √©l√©ment matrice: {layers[0][0].matrice[-2, ...]}\")\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                layers[layer][head].suppr_inf()\n",
    "                layers[layer][head].norm_tensor()\n",
    "                layers[layer][head].ecriture_xslx(crt=crt, ctx=full_ctx, precision= 4, absolute_folder=f\"{_OUTPUT_PATH}/full_matrice/{layer}\", filename=f\"{head}\", create_folder_path=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MultiEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : 1850\n",
      "dict_keys(['id', 'crt', 'ctxs', 'matrices', 'SL_matrice', 'heads'])\n",
      "[apr√®s] Snt(id=1850, tokens=['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<eos>'])\n",
      "[avant] Snt(id=1850, tokens=['In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<eos>'])\n",
      "[80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "Snt(id=1850, tokens=['When', 'women', 'flee', ',', 'other', 'reasons', 'are', 'in', 'the', 'foreground', ':', '&quot;', 'Many', 'women', 'sneak', 'away', 'secretly', ',', 'because', 'they', 'see', 'no', 'other', 'way', 'out', '.', '&quot;', '<eos>', '&quot;', 'Men', 'flee', 'because', 'of', 'war', ',', 'because', 'they', 'are', 'politically', 'persecuted', ',', 'because', 'they', 'are', 'threatened', 'with', 'torture', 'or', 'death', ',', 'or', 'because', 'their', 'families', 'pin', 'their', 'hopes', 'on', 'them', 'and', 'send', 'them', 'to', 'Europe', ',', '&quot;', 'she', 'explains', '.', '<eos>', '&quot;', 'I', '&apos;m', 'not', 'exaggerating', 'when', 'I', 'say', 'that', 'every', 'woman', 'who', 'arrives', 'here', 'has', 'dealt', 'with', 'sexual', 'violence', 'on', 'her', 'way', 'to', 'find', 'refuge', ',', '&quot;', 'said', 'Bahr', '.', '<eos>'])\n"
     ]
    }
   ],
   "source": [
    "import Utils_data as ud\n",
    "import Utils\n",
    "import torch\n",
    "import Classes.Matrice as Matrice\n",
    "import Classes.Sl_matrice as Sl_matrice\n",
    "from Classes.Snt import Snt\n",
    "\n",
    "precision = 8\n",
    "\n",
    "\n",
    "for id in range(1850, 1851):\n",
    "    print(f\"sentence : {id}\")\n",
    "    precision = 8\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/temp/han_attn2/{id}.json\"\n",
    "    OUTPUT_PATH = f\"/home/getalp/lopezfab/Documents/{id}\"\n",
    "\n",
    "    # Lecture des donn√©es\n",
    "    data=ud.lecture_data(r_path)\n",
    "    crt, ctxs, ctxs_heads, sl_heads = ud.lecture_multi_enc_objet(data)\n",
    "\n",
    "    # Corrections des donn√©es dans les cas o√π il y a moins de 3 contextes\n",
    "    mask = torch.ones(sl_heads[0].matrice.shape[1], dtype = torch.bool)\n",
    "    for k in range(len(ctxs)-1, -1, -1):\n",
    "        # On supprime les contextes inutiles\n",
    "        if len(ctxs[k].tokens) == 1 or (len(ctxs[k].tokens) > 1 and ctxs[k].tokens[-2] == \"<pad>\"):\n",
    "            del ctxs[k]\n",
    "            del ctxs_heads[k]\n",
    "            mask[k] = False\n",
    "        else:\n",
    "            # On corrige un probl√®me de padding qui apparait quand il y a moins de 3 contextes\n",
    "            for h in range(len(ctxs_heads[0])):\n",
    "                ctxs_heads[k][h].matrice = ctxs_heads[k][h].matrice[..., -len(ctxs[k]):]\n",
    "\n",
    "    # S'il y a au moins une phrase de contexte on l'a traite\n",
    "    if len(ctxs) >= 1:\n",
    "        for h in range(len(sl_heads)):\n",
    "            sl_heads[h].matrice = sl_heads[h].matrice[:, mask]\n",
    "        \n",
    "        # Process de la phrase courante\n",
    "        print(f\"[apr√®s] {crt}\")\n",
    "        list_crt_suppr_pad = crt.suppr_pad(strict=True)\n",
    "        print(f\"[avant] {crt}\")\n",
    "        print(list_crt_suppr_pad)\n",
    "        list_crt_fusion_bpe = crt.fusion_bpe()\n",
    "\n",
    "        mean_ctxs_heads = []\n",
    "        # Traitement de chaque contexte\n",
    "        for k in range(len(ctxs)):\n",
    "            # Process de la phrases de contexte K\n",
    "            list_ctx_suppr_pad = ctxs[k].suppr_pad(strict=True)\n",
    "            list_ctx_fusion_bpe = ctxs[k].fusion_bpe()\n",
    "\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                # Traitement de chaque t√™te d'attention entre la phrase courante et chaque phrase de contexte k\n",
    "                ctxs_heads[k][head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad, col_list_suppr_pad=  list_ctx_suppr_pad)\n",
    "                ctxs_heads[k][head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, col_list_fusion_bpe= list_ctx_fusion_bpe)\n",
    "                ctxs_heads[k][head].suppr_inf()\n",
    "\n",
    "            # Pour chaque phrase de contexte, on r√©cup√®re la moyenne des poids d'attention \n",
    "            # de la phrase courante vers la phrase de contexte (doit √™tre effectu√©e avant la normalisation )\n",
    "            mean_ctxs_heads.append(Matrice.Matrice(Utils.mean_matrices([ctxs_heads[k][head].matrice for head in range(len(ctxs_heads[k])) ])))\n",
    "            mean_ctxs_heads[k].norm_tensor() # On peut normaliser la moyenne car on ne l'utilise pas dans la contextualisation\n",
    "\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Traitement de chaque t√™te d'attention entre la phrase courante et l'ensemble des phrases de contexte\n",
    "            sl_heads[sl_head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad)\n",
    "            sl_heads[sl_head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, action= \"mean\")\n",
    "            \n",
    "\n",
    "        # On r√©cup√®re la moyenne des t√™tes d'attention du m√©canisme sentence level\n",
    "        mean_sl_heads = Sl_matrice.Sl_matrice(Utils.mean_matrices([sl_heads[sl_head].matrice for sl_head in range(len(sl_heads))]))\n",
    "        mean_sl_heads.norm_tensor() # On peut normaliser la moyenne car on l'utilise pas dans la contextualisation\n",
    "\n",
    "        \n",
    "        # Contextualisation entre le m√©canisme d'attention word-level et le m√©canisme d'attention sentence-level\n",
    "        # R√©cup√©ration de l'ensemble des phrases de contexte en une seule d'identifiant -1\n",
    "        full_ctx = Snt(identifiant=crt.identifiant, tokens=ctxs[0].tokens) if len(ctxs)>= 1 else None\n",
    "        if full_ctx is not None:\n",
    "            for k in range(1, len(ctxs)):\n",
    "                full_ctx.tokens += ctxs[k].tokens\n",
    "\n",
    "        # √©criture Token-level K x H x crt x ctxs[k] + means\n",
    "        for k in range(len(ctxs)):\n",
    "            # Pour chaque phrase de contexte K, \n",
    "            # On √©crit les matrices de chaque heads entre la phrase  courante et la phrase de contexte K\n",
    "            # Puis la moyenne de t√™tes\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                ctxs_heads[k][head].ecriture_xslx(crt= crt, \n",
    "                                                    ctx= ctxs[k],\n",
    "                                                    absolute_folder= f\"{OUTPUT_PATH}/token_level/{head}\", \n",
    "                                                    filename=f\"ctx_{k}\", \n",
    "                                                    precision=precision, \n",
    "                                                    create_folder_path=True)\n",
    "            mean_ctxs_heads[k].ecriture_xslx(crt= crt,\n",
    "                                                ctx= ctxs[k],\n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/token_level\", \n",
    "                                                filename=f\"mean_ctx_{k}\", \n",
    "                                                create_folder_path=True)\n",
    "\n",
    "        # √©criture Sentence-level H x crt x nb_ctx + means\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Pour chaque t√™te d'attention sentence-level,\n",
    "            # On √©crit la t√™te d'attention entre la phrase courante et les K phrases\n",
    "            # Puis on √©crit la moyenne des t√™tes d'attention\n",
    "            # TODO: √† v√©rifier si sl_heads est en mode k3 x k2 x k1 ou k1 x k2 x k3\n",
    "            # Si sl_heads est en mode  k1 x k2 x k3 alors .flip le passe en mode k3 x k2 x k1 pour plus de lisibilit√©\n",
    "            sl_heads[sl_head].matrice=sl_heads[sl_head].matrice.flip(1)\n",
    "            sl_heads[sl_head].ecriture_xslx(crt= crt, \n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                                filename=f\"sl_head_{sl_head}\", \n",
    "                                                precision=precision, \n",
    "                                                create_folder_path=True)\n",
    "        mean_sl_heads.ecriture_xslx(crt= crt,\n",
    "                                        absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                        filename=f\"mean\", \n",
    "                                        precision=precision, \n",
    "                                        create_folder_path=True)\n",
    "\n",
    "\n",
    "        print(full_ctx)\n",
    "        # Traitement et √©criture des phrases contextualis√© crt x nb_ctx\n",
    "        for h_sl in range(len(sl_heads)):\n",
    "            for h_tl in range(len(ctxs_heads[0])):\n",
    "                \n",
    "                temp = sl_heads[h_sl].contextualise_matrice([ctxs_heads[k][h_tl] for k in range(sl_heads[h_sl].matrice.size(1))])\n",
    "                temp.ecriture_xslx(crt= crt,\n",
    "                                            ctx= full_ctx,\n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/full_matrice/sl_{h_sl}\", \n",
    "                                            filename=f\"head_{h_tl}\", \n",
    "                                            precision=8,\n",
    "                                            create_folder_path=True)\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# OUTPUT_PATH/{id}/\n",
    "#  - sentence_level\n",
    "#     - sl_heads_{head}.xslx\n",
    "#        ...\n",
    "#  - token_level\n",
    "#     - {head}\n",
    "#        - ctx_{k}.xslx\n",
    "#           ...\n",
    "#  - full_matrice\n",
    "#     - sl_{sl_head}\n",
    "#        - head_{head}.xslt\n",
    "#           ...\n",
    "#        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une erreur s'est produite : [Errno 2] No such file or directory: '/home/getalp/lopezfab/Attention_git/marco_script/list_filename_matrice/k3_new_src_data.lst'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def write_file_paths_to_txt(directory, output_file):\n",
    "    \"\"\"\n",
    "    √âcrit les chemins absolus de tous les fichiers d'un dossier dans un fichier texte.\n",
    "\n",
    "    :param directory: Chemin du dossier √† parcourir.\n",
    "    :param output_file: Chemin du fichier texte de sortie.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w') as file:\n",
    "            for root, _, files in os.walk(directory):\n",
    "                for filename in files:\n",
    "                    absolute_path = os.path.abspath(os.path.join(root, filename))\n",
    "                    file.write(absolute_path + '\\n')\n",
    "        print(f\"Les chemins des fichiers ont √©t√© √©crits dans {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "directory_to_scan = \"/home/getalp/lopezfab/Documents/multienc\"\n",
    "output_txt_folder = \"/home/getalp/lopezfab/Attention_git/marco_script/list_filename_matrice\"\n",
    "output_txt_file = f\"{output_txt_folder}/k3_new_src_data.lst\"\n",
    "write_file_paths_to_txt(directory_to_scan, f\"{output_txt_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction Attention/process/5_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_snt = 102\n",
    "for k in range(1,3):\n",
    "    r_path = f\"/home/getalp/lopezfab/lig/Attention/concat/Process/post-processed_attn/5_output/tgt_side\"\n",
    "    w_path = f\"/home/getalp/lopezfab/lig/Attention/concat/Process/post-processed_attn/5_output_corrected/tgt_side\"\n",
    "    # Charger le fichier Excel existant\n",
    "    r_file_name = f\"{r_path}/tgt_{id_snt}_tgt_k{k}.tsv\"\n",
    "    w_file_name = f\"{r_path}/tgt_{id_snt}_tgt_k{k+1}.tsv\"\n",
    "    with open(r_file_name, 'r') as f:\n",
    "        data = f.read()\n",
    "    rows = data.split('\\n')\n",
    "    i_row = 0\n",
    "    lignes = []\n",
    "    while i_row in rows:\n",
    "        rows[i_row].split('\\t')\n",
    "        ligne = rows[i_row].split('\\t')\n",
    "        ligne[0][0] = f\"{id_snt}-k{k+1}\"\n",
    "        lignes.append(\"\\t\".join(ligne))\n",
    "        \n",
    "    with open(w_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lignes))\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
