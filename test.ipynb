{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Concaténation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] id: 188\n",
      "[['<eos>'], ['So', 'over', 'the', 'long', 'course', 'of', 'human', 'history', ',', 'the', 'infec@@', 'tious', 'disease', 'that', '&apos;s', 'killed', 'more', 'humans', 'than', 'any', 'other', 'is', 'malaria', '.', '<eos>'], ['It', '&apos;s', 'carried', 'in', 'the', 'bit@@', 'es', 'of', 'infected', 'mo@@', 'squ@@', 'it@@', 'os', ',', 'and', 'it', '&apos;s', 'probably', 'our', 'oldest', 's@@', 'cour@@', 'ge', '.', '<END>']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Snt import Snt\n",
    "from Matrice import Matrice\n",
    "import Utils_data\n",
    "import Utils_concat\n",
    "import importlib\n",
    "importlib.reload(Utils_data)\n",
    "importlib.reload(Utils_concat)\n",
    "_FULL_SNT = False # permet prendre en compte la phrase courante ou non\n",
    "id = 1850\n",
    "\n",
    "for id in range(188, 189):\n",
    "    print(f\"[DEBUG] id: {id}\")\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/test_attn/{id}.json\"\n",
    "    data=Utils_data.lecture_data(r_path)\n",
    "    _OUTPUT_PATH=f\"/home/getalp/lopezfab/Documents/concat/{id}\"\n",
    "\n",
    "    # Traitement des Phrases extraites\n",
    "    ssl = data['src_segments_labels']\n",
    "    src = Snt(identifiant=-1, tokens=Utils_concat.ajoute_eos_tokens_src(_snt=data[\"src_tokens\"].split(), src_segments_labels=ssl))\n",
    "    src_cutted = Utils_concat.full_sentence_to_ctx_and_crt(src)\n",
    "\n",
    "    # Au moins une phrase de contexte et la phrase courante (+ 1 car contient une phrase vide quand le nb de contexte est inférieur à la normale)\n",
    "    if len(src_cutted) > 2: \n",
    "        # Extraction des phrases de contexte et de la phrase courante\n",
    "        ctxs = []\n",
    "        for k in range(len(src_cutted[:-1])):\n",
    "            ctxs.append(Snt(identifiant=int(data[\"id\"]) - len(src_cutted[k:-1]), tokens = src_cutted[k]))\n",
    "        # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "\n",
    "        # Extraction des différentes matrices à travers les 6 layers et les 8 têtes d'attention de chaque layer\n",
    "        layers = []\n",
    "        for layer in range(len(data['heads_enc_attn'])): # Pour chaque layer\n",
    "            heads = []\n",
    "            for head in range(len(data['heads_enc_attn'][layer])): # on extrait chaque tête par layer\n",
    "                full_matrice = torch.tensor(data['heads_enc_attn'][layer][head])\n",
    "                full_matrice = full_matrice.squeeze() # on supprime une dimension qui semble inutile (=1)\n",
    "                heads.append(Matrice(full_matrice))\n",
    "            layers.append(heads)\n",
    "        # layers : L x [nb_heads x [torch.Tensor(N x N)]]\n",
    "\n",
    "        # Traitement du cas particulier où un contexte n'est pas présent. Suppression des reliquats dans le contexte et les matrices d'attention\n",
    "        for i in range(len(ctxs)-1, -1, -1):\n",
    "            if ctxs[i].tokens == [\"<eos>\"]:\n",
    "                del ctxs[i]\n",
    "                del src_cutted[i]\n",
    "                for layer in range(len(layers)):\n",
    "                    for head in range(len(layers[layer])):\n",
    "                        layers[layer][head].matrice = torch.cat([layers[layer][head].matrice[1:, 1:]])\n",
    "\n",
    "        # Récupération de l'ensemble des tokens\n",
    "        full_ctx = Snt(identifiant= id - len(ctxs), tokens= ctxs[0].tokens)\n",
    "        if len(ctxs) > 1:\n",
    "            for snt in ctxs[1:]:\n",
    "                full_ctx.tokens += snt.tokens\n",
    "                # print(f\"[debug] len(full_ctx): {len(full_ctx)}\")\n",
    "        crt = Snt(identifiant=data['id'], tokens= src_cutted[-1])\n",
    "        if _FULL_SNT: # permet prendre en compte la phrase courante ou non\n",
    "            full_ctx.tokens += crt.tokens\n",
    "\n",
    "        # print((f\"[DEBUG] taille des matrices: {layers[0][0].matrice.size()}\"))\n",
    "        # print(f\"[DEBUG] taille des phrases complète vs tailles respectives: {sum([len(ctx) for ctx in ctxs] + [len(crt)])} vs. [{[len(ctx) for ctx in ctxs]}, {len(crt)}]\")\n",
    "\n",
    "        # Pour chaque layer, pour chaque tête d'attention, on découpe la matrice en combinaison de k3*k3, k3*k2... k2*k3, k2*k2,... crt*crt\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                full_matrice_cutted = Utils_concat.cut_matrix_into_sentences(layers[layer][head], src_cutted)\n",
    "                # Dernière liste correspond à la phrase courante vers les phrases de contexte et la phrase courante\n",
    "                # _FULL_SNT permet prendre en compte la phrase courante ou non\n",
    "                if _FULL_SNT:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:]], dim = 1)\n",
    "                else:\n",
    "                    layers[layer][head].matrice = torch.cat([matrice.matrice for matrice in full_matrice_cutted[-1][:-1]], dim = 1)\n",
    "        # print(f\"[DEBUG] taille de la matrice découpée: {[matrice.matrice.size() for matrice in full_matrice_cutted[-1]]}\")\n",
    "        # print(f\"[DEBUG] taille de la matrice reconstituée: {layers[layer][head].matrice.size()}\")\n",
    "        # print(f\"[DEBUG] élément matrice: {layers[0][0].matrice[-2, ...]}\")\n",
    "        for layer in range(len(layers)):\n",
    "            for head in range(len(layers[layer])):\n",
    "                layers[layer][head].suppr_inf()\n",
    "                layers[layer][head].norm_tensor()\n",
    "                layers[layer][head].ecriture_xslx(crt=crt, ctx=full_ctx, precision= 4, absolute_folder=f\"{_OUTPUT_PATH}/full_matrice/{layer}\", filename=f\"{head}\", create_folder_path=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MultiEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : 1850\n",
      "dict_keys(['id', 'crt', 'ctxs', 'matrices', 'SL_matrice', 'heads'])\n",
      "[après] Snt(id=1850, tokens=['<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', 'In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<eos>'])\n",
      "[avant] Snt(id=1850, tokens=['In', 'their', 'home', 'countries', ',', 'they', 'experienced', 'sexual', 'violence', ',', 'forced', 'marriage', ',', 'honour', 'k@@', 'ill@@', 'ings', ',', 'sla@@', 'very', 'or', 'forced', 'prostitution', '.', '<eos>'])\n",
      "[80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "Snt(id=1850, tokens=['When', 'women', 'flee', ',', 'other', 'reasons', 'are', 'in', 'the', 'foreground', ':', '&quot;', 'Many', 'women', 'sneak', 'away', 'secretly', ',', 'because', 'they', 'see', 'no', 'other', 'way', 'out', '.', '&quot;', '<eos>', '&quot;', 'Men', 'flee', 'because', 'of', 'war', ',', 'because', 'they', 'are', 'politically', 'persecuted', ',', 'because', 'they', 'are', 'threatened', 'with', 'torture', 'or', 'death', ',', 'or', 'because', 'their', 'families', 'pin', 'their', 'hopes', 'on', 'them', 'and', 'send', 'them', 'to', 'Europe', ',', '&quot;', 'she', 'explains', '.', '<eos>', '&quot;', 'I', '&apos;m', 'not', 'exaggerating', 'when', 'I', 'say', 'that', 'every', 'woman', 'who', 'arrives', 'here', 'has', 'dealt', 'with', 'sexual', 'violence', 'on', 'her', 'way', 'to', 'find', 'refuge', ',', '&quot;', 'said', 'Bahr', '.', '<eos>'])\n"
     ]
    }
   ],
   "source": [
    "import Utils_data as ud\n",
    "import Utils\n",
    "import torch\n",
    "import Matrice\n",
    "import Sl_matrice\n",
    "from Snt import Snt\n",
    "\n",
    "precision = 8\n",
    "\n",
    "\n",
    "for id in range(1850, 1851):\n",
    "    print(f\"sentence : {id}\")\n",
    "    precision = 8\n",
    "    r_path=f\"/home/getalp/lopezfab/lig/temp/temp/temp/han_attn2/{id}.json\"\n",
    "    OUTPUT_PATH = f\"/home/getalp/lopezfab/Documents/{id}\"\n",
    "\n",
    "    # Lecture des données\n",
    "    data=ud.lecture_data(r_path)\n",
    "    crt, ctxs, ctxs_heads, sl_heads = ud.lecture_multi_enc_objet(data)\n",
    "\n",
    "    # Corrections des données dans les cas où il y a moins de 3 contextes\n",
    "    mask = torch.ones(sl_heads[0].matrice.shape[1], dtype = torch.bool)\n",
    "    for k in range(len(ctxs)-1, -1, -1):\n",
    "        # On supprime les contextes inutiles\n",
    "        if len(ctxs[k].tokens) == 1 or (len(ctxs[k].tokens) > 1 and ctxs[k].tokens[-2] == \"<pad>\"):\n",
    "            del ctxs[k]\n",
    "            del ctxs_heads[k]\n",
    "            mask[k] = False\n",
    "        else:\n",
    "            # On corrige un problème de padding qui apparait quand il y a moins de 3 contextes\n",
    "            for h in range(len(ctxs_heads[0])):\n",
    "                ctxs_heads[k][h].matrice = ctxs_heads[k][h].matrice[..., -len(ctxs[k]):]\n",
    "\n",
    "    # S'il y a au moins une phrase de contexte on l'a traite\n",
    "    if len(ctxs) >= 1:\n",
    "        for h in range(len(sl_heads)):\n",
    "            sl_heads[h].matrice = sl_heads[h].matrice[:, mask]\n",
    "        \n",
    "        # Process de la phrase courante\n",
    "        print(f\"[après] {crt}\")\n",
    "        list_crt_suppr_pad = crt.suppr_pad(strict=True)\n",
    "        print(f\"[avant] {crt}\")\n",
    "        print(list_crt_suppr_pad)\n",
    "        list_crt_fusion_bpe = crt.fusion_bpe()\n",
    "\n",
    "        mean_ctxs_heads = []\n",
    "        # Traitement de chaque contexte\n",
    "        for k in range(len(ctxs)):\n",
    "            # Process de la phrases de contexte K\n",
    "            list_ctx_suppr_pad = ctxs[k].suppr_pad(strict=True)\n",
    "            list_ctx_fusion_bpe = ctxs[k].fusion_bpe()\n",
    "\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                # Traitement de chaque tête d'attention entre la phrase courante et chaque phrase de contexte k\n",
    "                ctxs_heads[k][head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad, col_list_suppr_pad=  list_ctx_suppr_pad)\n",
    "                ctxs_heads[k][head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, col_list_fusion_bpe= list_ctx_fusion_bpe)\n",
    "                ctxs_heads[k][head].suppr_inf()\n",
    "\n",
    "            # Pour chaque phrase de contexte, on récupère la moyenne des poids d'attention \n",
    "            # de la phrase courante vers la phrase de contexte (doit être effectuée avant la normalisation )\n",
    "            mean_ctxs_heads.append(Matrice.Matrice(Utils.mean_matrices([ctxs_heads[k][head].matrice for head in range(len(ctxs_heads[k])) ])))\n",
    "            mean_ctxs_heads[k].norm_tensor() # On peut normaliser la moyenne car on ne l'utilise pas dans la contextualisation\n",
    "\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Traitement de chaque tête d'attention entre la phrase courante et l'ensemble des phrases de contexte\n",
    "            sl_heads[sl_head].suppr_pad(row_list_suppr_pad= list_crt_suppr_pad)\n",
    "            sl_heads[sl_head].fusion_bpe(row_list_fusion_bpe= list_crt_fusion_bpe, action= \"mean\")\n",
    "            \n",
    "\n",
    "        # On récupère la moyenne des têtes d'attention du mécanisme sentence level\n",
    "        mean_sl_heads = Sl_matrice.Sl_matrice(Utils.mean_matrices([sl_heads[sl_head].matrice for sl_head in range(len(sl_heads))]))\n",
    "        mean_sl_heads.norm_tensor() # On peut normaliser la moyenne car on l'utilise pas dans la contextualisation\n",
    "\n",
    "        \n",
    "        # Contextualisation entre le mécanisme d'attention word-level et le mécanisme d'attention sentence-level\n",
    "        # Récupération de l'ensemble des phrases de contexte en une seule d'identifiant -1\n",
    "        full_ctx = Snt(identifiant=crt.identifiant, tokens=ctxs[0].tokens) if len(ctxs)>= 1 else None\n",
    "        if full_ctx is not None:\n",
    "            for k in range(1, len(ctxs)):\n",
    "                full_ctx.tokens += ctxs[k].tokens\n",
    "\n",
    "        # écriture Token-level K x H x crt x ctxs[k] + means\n",
    "        for k in range(len(ctxs)):\n",
    "            # Pour chaque phrase de contexte K, \n",
    "            # On écrit les matrices de chaque heads entre la phrase  courante et la phrase de contexte K\n",
    "            # Puis la moyenne de têtes\n",
    "            for head in range(len(ctxs_heads[k])):\n",
    "                ctxs_heads[k][head].ecriture_xslx(crt= crt, \n",
    "                                                    ctx= ctxs[k],\n",
    "                                                    absolute_folder= f\"{OUTPUT_PATH}/token_level/{head}\", \n",
    "                                                    filename=f\"ctx_{k}\", \n",
    "                                                    precision=precision, \n",
    "                                                    create_folder_path=True)\n",
    "            mean_ctxs_heads[k].ecriture_xslx(crt= crt,\n",
    "                                                ctx= ctxs[k],\n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/token_level\", \n",
    "                                                filename=f\"mean_ctx_{k}\", \n",
    "                                                create_folder_path=True)\n",
    "\n",
    "        # écriture Sentence-level H x crt x nb_ctx + means\n",
    "        for sl_head in range(len(sl_heads)):\n",
    "            # Pour chaque tête d'attention sentence-level,\n",
    "            # On écrit la tête d'attention entre la phrase courante et les K phrases\n",
    "            # Puis on écrit la moyenne des têtes d'attention\n",
    "            # TODO: à vérifier si sl_heads est en mode k3 x k2 x k1 ou k1 x k2 x k3\n",
    "            # Si sl_heads est en mode  k1 x k2 x k3 alors .flip le passe en mode k3 x k2 x k1 pour plus de lisibilité\n",
    "            sl_heads[sl_head].matrice=sl_heads[sl_head].matrice.flip(1)\n",
    "            sl_heads[sl_head].ecriture_xslx(crt= crt, \n",
    "                                                absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                                filename=f\"sl_head_{sl_head}\", \n",
    "                                                precision=precision, \n",
    "                                                create_folder_path=True)\n",
    "        mean_sl_heads.ecriture_xslx(crt= crt,\n",
    "                                        absolute_folder= f\"{OUTPUT_PATH}/sentence_level\", \n",
    "                                        filename=f\"mean\", \n",
    "                                        precision=precision, \n",
    "                                        create_folder_path=True)\n",
    "\n",
    "\n",
    "        print(full_ctx)\n",
    "        # Traitement et écriture des phrases contextualisé crt x nb_ctx\n",
    "        for h_sl in range(len(sl_heads)):\n",
    "            for h_tl in range(len(ctxs_heads[0])):\n",
    "                \n",
    "                temp = sl_heads[h_sl].contextualise_matrice([ctxs_heads[k][h_tl] for k in range(sl_heads[h_sl].matrice.size(1))])\n",
    "                temp.ecriture_xslx(crt= crt,\n",
    "                                            ctx= full_ctx,\n",
    "                                            absolute_folder= f\"{OUTPUT_PATH}/full_matrice/sl_{h_sl}\", \n",
    "                                            filename=f\"head_{h_tl}\", \n",
    "                                            precision=8,\n",
    "                                            create_folder_path=True)\n",
    "\n",
    "\n",
    "## OUTPUT\n",
    "# OUTPUT_PATH/{id}/\n",
    "#  - sentence_level\n",
    "#     - sl_heads_{head}.xslx\n",
    "#        ...\n",
    "#  - token_level\n",
    "#     - {head}\n",
    "#        - ctx_{k}.xslx\n",
    "#           ...\n",
    "#  - full_matrice\n",
    "#     - sl_{sl_head}\n",
    "#        - head_{head}.xslt\n",
    "#           ...\n",
    "#        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
